---
title: "A Bayesian Framework for Persistent Homology"
subtitle: "An Introduction"
author:
  - name: Thomas Reinke
    affiliation: 
      - name: Baylor University
        department: Statistical Science
        # city: Waco
        # state: TX
        # country: US
        url: https://www.baylor.edu
    # email: thomas_reinke1@baylor.edu
date: today
date-format: "MMMM D, YYYY"
format: 
  revealjs:
    theme: 
      - quarto-assets/baylor-theme.scss
    smaller: false
    scrollable: false
    show-slide-number: all
    toc: false
    toc-depth: 1
    preview-links: true
    slide-number: c/t
    multiplex: true
    embed-resources: true
    auto-animate: true
    footer: "Thomas Reinke"
bibliography: references.bibtex
lightbox:
  match: auto
  effect: fade
  desc-position: bottom
  loop: true
logo: "quarto-assets/baylor.png"
license: "CC BY-NC"
copyright: 
  holder: Thomas Reinke
  year: 2025
editor: 
  markdown: 
    wrap: 72
---

```{r, setup}
#| include: false
#| message: false
library(knitr)
library(tidyverse)
library(conflicted)
library(janitor)
library(ggtda)
# library(TDAvis)
library(patchwork)
library(gganimate)
library(ggforce)
library(simplextree) 
library(gifski)
library(magick)  
library(ripserr)
library(BayesTDA)
library(TDAstats)
library(mvtnorm)
library(kableExtra)
library(plotly)
conflicted::conflict_prefer("filter", "dplyr")
conflicted::conflict_prefer("select", "dplyr")
conflicted::conflicts_prefer(ggtda::geom_simplicial_complex)
conflicted::conflicts_prefer(plotly::layout)
knitr::opts_chunk$set(
  comment = "#>",
  message = FALSE,
  warning = FALSE,
  cache = TRUE,
  echo = FALSE,
  tidy.opts = list(width.cutoff = 100),
  tidy = FALSE,
  fig.align = "center"
)
ggplot2::theme_set(ggplot2::theme_minimal())
ggplot2::theme_update(panel.grid.minor = ggplot2::element_blank())

#------------------------------------------------------------#
```

::: {.content-hidden}
$$
{{< include quarto-assets/_macros.tex >}}
$$
:::

# Contents

1.  [Introduction](#sec-intro)
1.  [Background](#sec-background)
1.  [Bayesian Framework](#sec-bayes)
1.  [Worked Example](#sec-example)
1.  [Classification](#sec-classification)
1.  [References](#sec-bib)


# Original Paper

A Bayesian Framework for Persistent Homology[@Maroulas2020-sp]


# Introduction {#sec-intro}


## Real World Problem

:::{.incremental}
- Motivation 
  - Detecting crystalline structures of materials HEAs(High Entropy Alloys)
:::

:::{.incremental}
- Current method APT(Atom Probe Tomography) 
    - 3D reconstruction of atoms
    - Drawbacks - experimental noise and abundance of missing data - 65% of atoms in a sample are not registered in typical experiment & spacial coordinates of atoms are corrupted by external noise. 
    - Existing algorithms rely on symmetry arguments and cannot establish crystal lattice
:::

## Real World Problem: Body Centered Cubic

```{r bcc}
bcc_atoms <- tribble(
  ~x,   ~y,   ~z,   ~type,     ~color_code,
  0,    0,    0,    "corner",  "green",     
  1,    0,    0,    "corner",  "orange",    
  0,    1,    0,    "corner",  "orange",    
  0,    0,    1,    "corner",  "beige",     
  1,    1,    0,    "corner",  "brown",     
  1,    0,    1,    "corner",  "beige",     
  0,    1,    1,    "corner",  "grey",      
  1,    1,    1,    "corner",  "blue",      
  0.5,  0.5,  0.5,  "center",  "green"      
)

bcc_cube_edges <- tribble(
  ~x1, ~y1, ~z1, ~x2, ~y2, ~z2,
  0,   0,   0,   1,   0,   0,
  0,   0,   0,   0,   1,   0,
  0,   0,   0,   0,   0,   1,
  1,   1,   0,   1,   0,   0,
  1,   1,   0,   0,   1,   0,
  0,   1,   1,   0,   0,   1,
  0,   1,   1,   0,   1,   0,
  1,   0,   1,   1,   0,   0,
  1,   0,   1,   0,   0,   1,
  1,   1,   1,   1,   1,   0,
  1,   1,   1,   0,   1,   1,
  1,   1,   1,   1,   0,   1
)

center_atom <- bcc_atoms |> filter(type == "center")
corner_atoms <- bcc_atoms |> filter(type == "corner")

center_edges <- corner_atoms |>
  select(x2 = x, y2 = y, z2 = z) |>
  mutate(
    x1 = center_atom$x,
    y1 = center_atom$y,
    z1 = center_atom$z
  ) |>
  select(x1, y1, z1, x2, y2, z2) 

all_edges <- bind_rows(bcc_cube_edges, center_edges)

edge_data <- all_edges |>
  rowwise() |>
  mutate(
    x = list(c(x1, x2, NA)),
    y = list(c(y1, y2, NA)),
    z = list(c(z1, z2, NA))
  ) |>
  ungroup() |>
  summarise(
    x = list(unlist(x)),
    y = list(unlist(y)),
    z = list(unlist(z))
  ) |>
  unnest(cols = c(x, y, z)) 

plot_ly() |>
  add_trace(
    data = bcc_atoms |> filter(type == "corner"),
    x = ~x, y = ~y, z = ~z,
    type = "scatter3d", mode = "markers",
    marker = list(
        color = ~color_code, 
        size = 8,
        line = list(color = 'black', width = 1) 
        ),
    name = "Corner Atoms" 
  ) |>
  add_trace(
    data = bcc_atoms |> filter(type == "center"),
    x = ~x, y = ~y, z = ~z,
    type = "scatter3d", mode = "markers",
    marker = list(
        color = ~color_code,
        size = 10,
        line = list(color = 'black', width = 1) 
        ),
    name = "Center Atom" 
  ) |>
  add_trace(
    data = edge_data,
    x = ~x, y = ~y, z = ~z,
    type = "scatter3d", mode = "lines",
    line = list(color = "grey50", width = 2), 
    name = "Edges",
    showlegend = FALSE 
  ) |>
  plotly::layout(
    title = "BCC Structure",
    scene = list(
      xaxis = list(title = 'X', range = c(-0.2, 1.2), showgrid=FALSE, zeroline=FALSE, showticklabels=FALSE, titlefont=list(size=12)),
      yaxis = list(title = 'Y', range = c(-0.2, 1.2), showgrid=FALSE, zeroline=FALSE, showticklabels=FALSE, titlefont=list(size=12)),
      zaxis = list(title = 'Z', range = c(-0.2, 1.2), showgrid=FALSE, zeroline=FALSE, showticklabels=FALSE, titlefont=list(size=12)),
      aspectmode = 'cube',
      camera = list(eye = list(x=1.5, y=1.5, z=1.5))
    ),
    showlegend = FALSE
  )
```


## Real World Problem: Face Centered Cubic

```{r fcc}
fcc_atoms <- tribble(
  ~x,   ~y,   ~z,   ~type,          ~color_code, ~id,
  0,    0,    0,    "corner",       "beige",     "C000",  
  1,    0,    0,    "corner",       "green",     "C100",   
  0,    1,    0,    "corner",       "orange",    "C010",  
  0,    0,    1,    "corner",       "brown",     "C001",
  1,    1,    0,    "corner",       "green",     "C110",   
  1,    0,    1,    "corner",       "grey",      "C101",  
  0,    1,    1,    "corner",       "blue",      "C011",  
  1,    1,    1,    "corner",       "blue",      "C111",  
  0.5,  0.5,  0,    "face_center",  "beige",     "Fxy0",   
  0.5,  0,    0.5,  "face_center",  "beige",     "Fxz0",   
  0,    0.5,  0.5,  "face_center",  "orange",    "Fyz0",  
  1,    0.5,  0.5,  "face_center",  "brown",     "Fyz1",   
  0.5,  1,    0.5,  "face_center",  "orange",    "Fxz1",   
  0.5,  0.5,  1,    "face_center",  "brown",     "Fxy1"    
) |>
  mutate(id = as.character(id))

fcc_cube_edges <- tribble(
  ~x1, ~y1, ~z1, ~x2, ~y2, ~z2,
  0,   0,   0,   1,   0,   0,
  0,   0,   0,   0,   1,   0,
  0,   0,   0,   0,   0,   1,
  1,   1,   0,   1,   0,   0,
  1,   1,   0,   0,   1,   0,
  0,   1,   1,   0,   0,   1,
  0,   1,   1,   0,   1,   0,
  1,   0,   1,   1,   0,   0,
  1,   0,   1,   0,   0,   1,
  1,   1,   1,   1,   1,   0,
  1,   1,   1,   0,   1,   1,
  1,   1,   1,   1,   0,   1
)

face_centers <- fcc_atoms |> filter(type == "face_center")
corners <- fcc_atoms |> filter(type == "corner")

get_face_corners <- function(fc_x, fc_y, fc_z) {
  if (abs(fc_z - 0) < 1e-6) { 
    corners |> filter(abs(z - 0) < 1e-6)
  } else if (abs(fc_z - 1) < 1e-6) { 
    corners |> filter(abs(z - 1) < 1e-6)
  } else if (abs(fc_y - 0) < 1e-6) { 
    corners |> filter(abs(y - 0) < 1e-6)
  } else if (abs(fc_y - 1) < 1e-6) { 
    corners |> filter(abs(y - 1) < 1e-6)
  } else if (abs(fc_x - 0) < 1e-6) { 
    corners |> filter(abs(x - 0) < 1e-6)
  } else if (abs(fc_x - 1) < 1e-6) { 
    corners |> filter(abs(x - 1) < 1e-6)
  } else {
    tibble()
  }
}

face_corner_edges_list <- list()
for (i in 1:nrow(face_centers)) {
  fc_row <- face_centers[i, ]
  connected_corners <- get_face_corners(fc_row$x, fc_row$y, fc_row$z)

  if (nrow(connected_corners) > 0) {
    face_corner_edges_list[[i]] <- connected_corners |>
      select(x2 = x, y2 = y, z2 = z) |> 
      mutate(
        x1 = fc_row$x, 
        y1 = fc_row$y,
        z1 = fc_row$z
      ) |>
      select(x1, y1, z1, x2, y2, z2) 
  }
}
face_corner_edges <- bind_rows(face_corner_edges_list)

all_edges <- bind_rows(fcc_cube_edges, face_corner_edges)

edge_data <- all_edges |>
  rowwise() |>
  mutate(
    x = list(c(x1, x2, NA)), 
    y = list(c(y1, y2, NA)),
    z = list(c(z1, z2, NA))
  ) |>
  ungroup() |>
  summarise(
    x = list(unlist(x)),
    y = list(unlist(y)),
    z = list(unlist(z))
  ) |>
  unnest(cols = c(x, y, z))


plot_ly(showlegend = FALSE) |> 
  add_trace(
    data = fcc_atoms,
    x = ~x, y = ~y, z = ~z,
    type = "scatter3d", mode = "markers",
    marker = list(
        color = ~color_code, 
        # size = ~ifelse(type == "corner", 8, 7), 
        size = 10,
        line = list(color = 'black', width = 1) 
        ),
    hoverinfo = 'text', 
    text = ~paste("Type:", type, "<br>Coord: (", x, ",", y, ",", z, ")<br>Color:", color_code)
  ) |>
  add_trace(
    data = edge_data,
    x = ~x, y = ~y, z = ~z,
    type = "scatter3d", mode = "lines",
    line = list(color = "green", width = 1.5), 
    name = "Edges",
    hoverinfo = 'none' 
  ) |>
  plotly::layout(
    title = "FCC Structure",
    scene = list(
      xaxis = list(title = 'X', range = c(-0.2, 1.2), showgrid=FALSE, zeroline=FALSE, showticklabels=FALSE, titlefont=list(size=12)),
      yaxis = list(title = 'Y', range = c(-0.2, 1.2), showgrid=FALSE, zeroline=FALSE, showticklabels=FALSE, titlefont=list(size=12)),
      zaxis = list(title = 'Z', range = c(-0.2, 1.2), showgrid=FALSE, zeroline=FALSE, showticklabels=FALSE, titlefont=list(size=12)),
      aspectmode = 'cube',
      camera = list(eye = list(x=1.5, y=1.5, z=1.5)) 
    )
  )
```

## TDA Motivation

:::{.incremental}
- Explore topological structure in datasets
    - Persistent homolgy, associate shapes to data & summarize  features with persistence diagrams
:::

:::{.incremental}
- Applicable to many areas
    - handwriting analysis, brain arteries, image analysis, neuroscience, sensor networks, protein structures, biology, dynamical systems, action recognition, signal analysis, chemistry, genetics, object data, etc.
:::

## TDA Motivation

:::{.fragment}
- Persistence Diagrams (PDs) for inference & classification problems
:::

:::{.incremental}
- Model PDs as random sets(not vectors) - Poisson point processes - single parameter intensity
    - Derive closed form of posterior - relies on conjugate families of Gaussian mixtures
:::

:::{.notes}
- compute posterior intensities w/o explicit maps between input diagram & underlying parameters
- Random vectors $X = \{X_1, \ldots, X_n\}$
- Random set - length not necessarily the same for each realization, and order doesn't matter
:::


# Background {#sec-background}

## Example Data

```{r ex_df}
set.seed(1)

p1 <- tdaunif::sample_circle(n = 40L, sd = .05)
p2 <- tdaunif::sample_circle(n = 20L, sd = .025) / 2
colnames(p1) <- colnames(p2) <- c("x", "y")
p2[, "x"] <- p2[, "x"] + .5

df <- rbind(
  cbind(as.data.frame(p1), lab = "a"),
  cbind(as.data.frame(p2), lab = "b")
)

ggplot(df, aes(x, y)) +
  geom_point() +
  coord_fixed()
```

## Persistence Diagrams

Fundamental geometric structures

:::{.fragment}
* **Simplices:** Basic shapes like points, line segments, triangles, tetrahedra.
:::

:::{.fragment}
* **Simplicial Complexes:** Collections of simplices "glued" together in a structured way.
:::

## Simplices: Geometric Building Blocks {.smaller}

* A **$k$-simplex** is the convex hull of $k+1$ geometrically independent points (vertices) $\{v_0, ..., v_k\}$.
    * Notation: $[v_0, ..., v_k] = \bigl\{ \sum_{i=0}^k \alpha_i v_i : \sum_{i=0}^k \alpha_i = 1, \alpha_i \ge 0 \bigr\}$

::: {.incremental}
* **Examples:**
    * 0-simplex: $[v_0]$ (a point/vertex)
    * 1-simplex: $[v_0, v_1]$ (a line segment/edge)
    * 2-simplex: $[v_0, v_1, v_2]$ (a triangle, including interior)
    * 3-simplex: $[v_0, v_1, v_2, v_3]$ (a tetrahedron, including interior)
:::

:::{.fragment}
* **Faces:** k-1 simplices spanned by subsets of the vertices. (e.g., edges of a triangle).
:::

:::{.notes}
- $\alpha_i$ is weights/coefficients that sum to one, creates the hull/filled-in geometric object
- The convex hull is the smallest convex set that encloses all the points, forming a convex polygon
- "geometrically independent" means the points are affinely independent
- ex 3 points, not collinear
:::


## Simplicial Complexes

A **Simplicial Complex** $S$ is a collection of simplices satisfying:

:::{.fragment}
1.  **Face Inclusion:** If a simplex $\xi \in S$, then all faces of $\xi$ are also in $S$.
:::
:::{.incremental}
2.  **Consistent Intersection:** The intersection of any two simplices $\xi_1, \xi_2 \in S$ is either:
    * Empty ($\emptyset$), or
    * Contained in $S$
:::

:::{.fragment}
* **Intuition**: A well-constructed mesh made of points, edges, triangles, etc.
:::

## Example 

<!-- This is the simplicial complex generated by $r = .25$ -->

```{r ex_sc, fig.align='center'}

radius <- .25

prox <- 2 * radius

p_d <- ggplot(df, aes(x = x, y = y)) +
  coord_fixed() +
  geom_disk(radius = radius, fill = "aquamarine3") +
  geom_point() +
  theme_bw()

p1 <- ggplot(df, aes(x, y)) +
  geom_simplicial_complex(radius = .25) +
  coord_fixed()

p2 <- ggplot(df, aes(x, y)) +
  geom_simplicial_complex(
    mapping = aes(fill = after_stat(face)),
    dimension_max = 4L,
    alpha = .25, radius = .25
  ) +
  coord_fixed()

# p3 <- ggplot(df, aes(x, y, fill = lab)) +
#   geom_simplicial_complex(radius = .3) +
#   coord_fixed()
# 
# p4 <- ggplot(df, aes(x, y)) +
#   stat_simplicial_complex(
#     mapping = aes(fill = after_stat(factor(dimension)),
#                   color = after_stat(factor(dimension))),
#     dimension_max = 6L,# one_simplices = "all",
#     alpha = .5, radius = .3
#   ) +
#   facet_wrap(vars(lab), ncol = 2) +
#   scale_fill_viridis_d(direction = -1, aesthetics = c("color", "fill")) +
#   coord_fixed()

p_d + p1

# p1 + p2 + p3 + p4 +
#   plot_layout(
#     nrow = 2,
#     ncol = 2
#   )
  

```

## Vietoris-Rips

:::{.incremental}
* **Goal:** Build a simplicial complex from a point cloud $X = \{x_i\}_{i=1}^L \subset \mathbb{R}^d$.
* **Vietoris-Rips Complex $\mathcal{V}_r(X)$:**
    * Parameter: Proximity radius $r > 0$.
    * Rule: A simplex $[x_{i_1}, ..., x_{i_l}] \in \mathcal{V}_r(X)$ iff $\text{diam} (x_{i_1}, ..., x_{i_l}) \leq r$
:::

:::{.fragment}
* **Intuition:** Connect groups of points that are close to each other.
:::

:::{.notes}
- $\text{diam}$ means the greatest distance between any 2 points
:::

## VR Filtration

:::{.fragment}
* Given a nondecreasing sequence $\{ r_n\}  \in  \mathbb{R}^+ \cup  \{ 0\}$  with $r_0 = 0$, we denote its Vietoris--Rips filtration by $\{ \mathcal{V}_{r_n} (X)\}_{ n\in \mathbb{N}}$ .
:::

:::{.fragment}
* A **Vietoris-Rips Filtration** is a sequence of nested simplicial complexes:
    $$ \mathcal{V}_{r_0}(X) \subseteq \mathcal{V}_{r_1}(X) \subseteq \mathcal{V}_{r_2}(X) \subseteq \dots $$
:::

:::{.fragment}
* As we increase the radius $r$, more points get connected, and higher-dimensional simplices appear.    
:::

:::{.fragment}
* This filtration tracks how the topological structure (connectivity, holes, voids) evolves as the scale $r$ changes.
:::

## VR Visualization

```{r vr_vis}
# Define the sequence of radii
# radii_seq <- seq(0, .35, length.out = 201)
# 
# create_plot <- function(r, data) {
#   ggplot(data, aes(x = x, y = y)) +
#     coord_fixed() +
#     geom_disk(radius = r, fill = "aquamarine3", alpha = 0.5) + 
#     geom_simplicial_complex(radius = r) +
#     geom_point() +
#     labs(title = paste("Radius:", format(r, digits = 2))) + 
#     theme_bw()
# }
# 
# plot_list <- map(radii_seq, ~ create_plot(.x, data = df))
# 
# temp_dir <- tempdir()
# file_paths <- file.path(temp_dir, paste0("frame_", seq_along(plot_list), ".png"))
# 
# pwalk(list(plot_list, file_paths),
#       ~ ggsave(filename = ..2, plot = ..1, width = 10, height = 5, dpi = 500))
#
# img_list <- map(file_paths, image_read)
# 
# img_joined <- image_join(img_list)
# 
# animation <- image_animate(img_joined, fps = 20)
# 
# # print(animation)
# 
# output_gif_path <- "radius_animation.gif"
# image_write(animation, path = output_gif_path)
# 
# unlink(file_paths) 

```

![](radius_animation.gif)

## Persistence Diagrams (PDs) {.smaller}

:::{.incremental}
* A Persistence Diagram (PD) $\mathcal{D}$ is a multiset of points in the space $\mathcal{W} = \mathbb{W} \times \{0, 1, ..., D-1\}$.
    * The **birth-death plane** is $\mathbb{W} = \{ (b, d) \in \mathbb{R}^2 \mid d \ge b \ge 0 \}$.
:::

:::{.incremental}
* Each point $(b, d, k) \in \mathcal{D}$ represents:
    * A homological feature of dimension $k$.
    * The feature appears (is **b**orn) at filtration scale $b$.
    * The feature disappears ( **d**ies) at filtration scale $d$.
:::


:::{.incremental}
* **Persistence:** $d-b$ (how long the feature "persists"). Longer persistence often indicates significant features.
* **Dimensions:**
    * $k=0$: Connected components.
    * $k=1$: Loops or holes.
    * $k=2$: Voids or cavities.
:::

:::{.notes}
A **Persistence Diagram** $\mathcal{D}$ summarizes the topological features found in a filtration.
- homological feature dim $k$ appears at $b$ and disappears at $d$
- dies when merges with another feature, ex loop dies when 'filled in' by triangles
- void - empty sphere, like a basketball
:::

## Persistence Diagrams

```{r pd_diagram}
rips_data <- ripserr::vietoris_rips(df[, c("x", "y")])

barcode_graph <- ggplot(rips_data) +
  geom_barcode(
    aes(start = birth,
        end = death,
        color = factor(dimension),
        linetype = factor(dimension)),
    linewidth = 1.2  
  ) +
  scale_color_viridis_d(end = .7, name = "Dimension") +
  scale_linetype_discrete(name = "Dimension") +
  labs(title = "Barcode")

pd_graph <- ggplot(rips_data) +
  stat_persistence(
    aes(start = birth, end = death,
        color = factor(dimension), shape = factor(dimension)),
    size = 2.5,
    alpha = .75
  ) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", alpha = 0.6) +
  scale_color_viridis_d(end = .7, name = "Dimension") +
  scale_shape_discrete(name = "Dimension") +
  labs(title = "Persistence Diagram")

barcode_graph + pd_graph +
  plot_layout(guides = 'collect') &
  theme(legend.position = 'bottom')
```

## The Poisson Point Process (PPP) {.smaller}

<!-- * **Goal:** Model random collections of points $\{x_1, \dots, x_n\}$ in a space $\mathbb{X}$ where order doesn't matter. *(Think of Persistence Diagrams as examples of such collections)*. -->

::: {.incremental}
* **General Idea:** Finite Point Processes $(\{p_n\}, \{\mathbb{P}_n\})$ model the *number* of points ($N \sim \{p_n\}$) and their *spatial distribution* ($(x_1, \dots, x_n) \sim \mathbb{P}_n$).
:::

::: {.incremental}
* **Poisson PP $\Pi$:** A specific, widely used PP governed by an **Intensity Measure $\Lambda$** on $\mathbb{X}$. Let $\mu = \Lambda(\mathbb{X})$.
    * **Cardinality ($N$):** The number of points $N \sim \text{Poisson}(\mu)$.
    * **Spatial Distribution ($\mathbb{P}_n$):** Given $N=n$ points, their locations are drawn **independently** from the normalized intensity measure $\frac{\Lambda(\cdot)}{\mu}$.
      $$ \mathbb{P}_n(A_1 \times \dots \times A_n) = \prod_{i=1}^n \left( \frac{\Lambda(A_i)}{\mu} \right) $$
:::

::: {.incremental}
* **Key Property:** The intensity $\Lambda(A)$ is the **expected number** of points falling within a region $A \in \mathcal{X}$:
    $$ \mathbb{E}[|\Pi \cap A|] = \Lambda(A) $$
:::

::: {.notes}
* $\mathbb{X}$ is polish space
* $\mathbb{P_n}$ symmetricy probablility measure on $X^n$ which is the Borel $\sigma$ algebra of $\mathbb{X}$
* **Why PPPs?** We're using PPPs because they provide a flexible framework for modeling point patterns like Persistence Diagrams, where both the *number* of points (topological features) and their *locations* (birth/death times) are random.
* **Intensity Measure $\Lambda$**: This is the core object defining a PPP.
    * It's a measure on the underlying space $\mathbb{X}$.
    * It controls *where* points are likely to appear. Regions $A$ with higher $\Lambda(A)$ are expected to contain more points.
    * Often, $\Lambda$ has a density $\lambda(x)$ (the intensity *function*), so $\Lambda(A) = \int_A \lambda(x) dx$. $\lambda(x)$ represents the "rate" or "density" of points near $x$.
    * The total intensity $\mu = \Lambda(\mathbb{X})$ is the *expected total number* of points in the process.
* **Cardinality:** Unlike a fixed dataset, the total number of points $N$ in a *realization* of a PPP is random, following a Poisson distribution with mean $\mu$.
* **Spatial Distribution:** This is crucial. For a *given* number of points $n$, the PPP places them *independently* of each other. The probability of a point landing in a region $A$ is proportional to $\Lambda(A)$. Knowing a point exists at $x$ gives no extra information about points nearby, beyond what $\Lambda$ already tells us.
* **Key Property:** This equation ($\mathbb{E}[|\Pi \cap A|] = \Lambda(A)$) directly connects the mathematical intensity measure $\Lambda$ to a tangible meaning: the average number of points you'd find in any region $A$. This justifies calling $\Lambda$ the "intensity".
:::

## Marked Poisson Point Processes {.smaller}

* **Motivation:** Model points $x \in \mathbb{X}$ that have associated attributes or 'marks' $m$ from another space $\mathbb{M}$. *(e.g., PD points $x$ with marks $m$ describing feature properties)*.

::: {.incremental}
* **Definition:** A Marked PPP $\Pi_M$ is a Point Process living on the product space $\mathbb{X} \times \mathbb{M}$.
:::

::: {.incremental}
* **Key Components & Properties:**
    * Relies on an underlying standard PPP on $\mathbb{X}$ (intensity $\Lambda$) to place the points $x$.
    * Uses a **Stochastic Kernel $\ell$**: For any fixed location $x \in \mathbb{X}$, $\ell(x, \cdot)$ is a **probability measure on the mark space $\mathbb{M}$**, defining the distribution $P(\text{mark } | \text{ location } x)$.
    * Given points $\{x_1, \dots, x_n\}$, their marks $\{m_1, \dots, m_n\}$ are generated **conditionally independently**, with $m_i$ drawn from the distribution $\ell(x_i, \cdot)$.
:::

::: {.notes}
* **Intuition:** Think of a two-stage process:
    1. Generate point locations $\{x_i\}$ in $\mathbb{X}$ using a standard PPP.
    2. For each generated point $x_i$, independently generate its mark $m_i$ using the probability distribution defined by the kernel $\ell(x_i, \bullet)$.
* $\mathbb{M}$ is the dimension of the birth death points
* **Stochastic Kernel $\ell$:** This is the crucial link. For each specific location $x$, $\ell(x, \bullet)$ *is* a complete probability measure defined over all possible marks in $\mathbb{M}$. It tells you the probability of getting any particular mark (or set of marks) given you are at location $x$. This means $\int_{\mathbb{M}} \ell(x, dm) = 1$.
* **Result:** The final output $\Pi_M$ is a collection of pairs $(x_i, m_i)$ living in the combined space $\mathbb{X} \times \mathbb{M}$.
* **Usefulness:** This structure allows modeling situations where point locations influence their properties, or where we might observe properties (marks) and want to infer locations (basis for later Bayesian section).
:::


# Bayesian Framework {#sec-bayes}

## Setting the Stage: Representation & Goal

:::{.fragment}
* **Tilted Representation:** We work with the "tilted" PD representation $(b, d-b)$ instead of $(b, d)$. This space is denoted $\mathbb{W}$. *(Notation Abuse: $\mathcal{D}$ is used for the tilted version $T(\mathcal{D})$).*
:::

:::{.fragment}
* **Focus:** We analyze one homological dimension $k$ at a time, assuming independence between dimensions (Model M1). *(Notation Abuse: We write $\mathcal{D}_X$ for $\mathcal{D}^k_X$)*
:::

:::{.fragment}
* **Goal:** Develop a Bayesian framework to update our belief about a "true" underlying (latent) PD, represented by a prior intensity $\lambda$, using one or more *observed* PDs $\mathcal{D}_Y$.
:::

::: {.notes}
* **Tilted Representation:** First, a quick coordinate change. Instead of birth ($b$) and death ($d$), we'll use birth ($b$) and persistence ($p = d-b$). This is just a linear transformation, often convenient for modeling. We denote the space of these $(b, p)$ points as $\mathbb{W}$. We'll often just write $\mathcal{D}$ when we mean the set of these tilted points.
* **Focus on One Dimension (M1):** A key simplifying assumption (Model M1 from the paper) is that topological features across different homological dimensions (e.g., $H_0$ components vs. $H_1$ loops vs. $H_2$ voids) are statistically independent. This allows us to build our entire framework for **one fixed dimension $k$** at a time.
* **Notation Abuse:** Because we're focusing on a single, fixed $k$ for the whole analysis (M1, M2, M3, Theorem 3.1), we'll drop the subscript $k$. So, when you see $\mathcal{D}_X$ or $\mathcal{D}_Y$, remember this implicitly means $\mathcal{D}^k_X$ (the latent diagram for dimension $k$) and $\mathcal{D}^k_Y$ (the observed diagram for dimension $k$).
* **The Goal:** We aim to perform Bayesian inference. We start with a *prior* belief about the "true" underlying PD for our chosen dimension $k$. This belief is encoded by modeling the latent PD, $\mathcal{D}_X$, as a Poisson Point Process (PPP) with a spatial **prior intensity** $\lambda(x)$. Then, we use *observed* data – specifically, one or more persistence diagrams $\mathcal{D}_Y$ (if we have multiple datasets $i=1, ..., m$, we'll use $D_{Y^1}, ..., D_{Y^m}$, all computed for the **same dimension $k$**) – to update our prior belief into a **posterior intensity**.
:::

## Bayesian Framework: Analogy

|      | Bayesian framework for RVs | Bayesian framework for random PDs   |
| :------------- | :-------------------------------- | :---------------------------------------------------------- |
| **Prior** | Modeled by a prior density $f$    | Modeled by a Poisson PP with prior intensity $\lambda$        |
| **Likelihood** | Depends on observed data          | Stochastic kernel $\color{purple}{\ell(y|x)}$ depends on observed PDs |
| **Posterior** | Compute the posterior density     | Defines a Poisson PP with posterior intensity               |

:::{.fragment}
* **Core Idea:** Treat unknown, latent PD $\mathcal{D}^X$ as random draw from PPP defined by *prior* intensity $\lambda_{\mathcal{D}^X}$. Use observed PDs $D_{Y^1}, ..., D_{Y^m}$ (for fixed $k$) and likelihood kernel $\color{purple}{\ell(y|x)}$ to get *posterior* intensity $\lambda_{\mathcal{D}_X | D_{Y^{1:m}}}$.
:::

::: {.notes}
**Speaker Notes:**

* This table draws parallels between standard Bayesian inference (for parameters $\theta$ of random variables) and our proposed framework for persistence diagrams (PDs). Remember, we're focusing on **one fixed homological dimension $k$**.
* **Prior Belief:**
    * *Standard:* We express prior belief about a parameter $\theta$ using a probability density function $f(\theta)$.
    * *PD Framework:* Our prior belief about the *latent* PD $\mathcal{D}^X$ (really $\mathcal{D}^k_X$) is encoded by modeling it as a **Poisson Point Process (PPP)**. The key characteristic of this PPP is its **prior intensity function $\lambda(x)$** (short for $\lambda_{\mathcal{D}^k_X}(x)$). This function tells us the expected density of "true" latent features at location $x=(b, p)$ in the persistence plane $\mathbb{W}$.
* **Data Connection (Likelihood):**
    * *Standard:* The likelihood function $L(\text{data}|\theta)$ quantifies how probable the observed data is, given a specific parameter value $\theta$.
    * *PD Framework:* The link between the latent features $x$ (from $\mathcal{D}^k_X$) and the observed features $y$ (in $\mathcal{D}^k_Y$) is defined by a **stochastic kernel** $\color{purple}{\ell(y|x)}$. This kernel acts like our likelihood: it gives the probability density of observing a feature $y$ *assuming* it originated from a specific latent feature $x$. The actual data we use are the points in the observed diagram(s) $D_{Y^1}, \dots, D_{Y^m}$. (This corresponds to the `purple` term in the theorem).
* **Updated Belief (Posterior):**
    * *Standard:* Combining the prior $f(\theta)$ and likelihood $L(\text{data}|\theta)$ via Bayes' theorem gives the posterior density $f(\theta|\text{data})$.
    * *PD Framework:* Combining the prior intensity $\lambda(x)$ and the likelihood kernel $\color{purple}{\ell(y|x)}$ using the observed data $D_{Y^{1:m}} = \cup_{i=1}^m D_{Y^i}$ yields the **posterior intensity function** $\lambda(x | D_{Y^{1:m}})$. This function defines a *posterior* Poisson Point Process, which represents our updated probabilistic belief about the latent PD $\mathcal{D}^k_X$ after seeing the data. (The formula involves the `red`, `blue`, `purple`, and `darkgreen` components).
* **Core Idea Summary:** We model the unknown true PD (for dimension $k$) as a PPP realization governed by a prior intensity $\lambda$. We use the actual points from observed diagrams ($D_{Y^1}, \dots, D_{Y^m}$, all for dim $k$) and a kernel $\color{purple}{\ell(y|x)}$ (our likelihood) to compute a posterior intensity. This posterior intensity defines the PPP representing our updated understanding of the true PD structure.
:::

## The Model: Latent PD $\mathcal{D}_X$ {.smaller}

* The "true" underlying PD $\mathcal{D}_X$ is modeled as a **Poisson Point Process (PPP)** with a **prior intensity density** $\lambda_{\mathcal{D}_X}(x)$.

::: {.incremental}
* Any potential feature $x$ from the prior might be **observable** or **vanish** (unobserved).
    * $\alpha(x)$: Probability that feature $x$ is potentially observable.
* $\mathcal{D}_X$ is decomposed into two *independent* PPPs:
    * $\mathcal{D}_{XO}$ (**O**bserved Track): Points that *could* generate an observation.
        * Intensity: $\color{blue}{\alpha(x) \lambda_{\mathcal{D}_X}(x)}$
    * $\mathcal{D}_{XV}$ (**V**anished Track): Points that are missed / not observed.
        * Intensity: $\color{red}{(1-\alpha(x)) \lambda_{\mathcal{D}_X}(x)}$
:::

::: {.notes}
* This slide describes Model M2, which defines our prior belief about the **latent ("true") persistence diagram**, $\mathcal{D}^X$.
* **Remember the Context (M1):** We are operating under Model M1, meaning we've fixed a specific **homological dimension $k$**. So, $\mathcal{D}_X$ here is technically $\mathcal{D}^k_X$, representing the true features only for that dimension $k$.
* **Prior Model:** We model this latent $\mathcal{D}_X$ as a **Poisson Point Process (PPP)** on the persistence plane $\mathbb{W}$. The behavior of this PPP is governed by its **prior intensity density** $\lambda_{\mathcal{D}_X}(x)$ (really $\lambda_{\mathcal{D}^k_X}(x)$). This function reflects our prior assumptions about the density of true dimension-$k$ features at location $x=(b, p)$.
* **Detectability** $\alpha(x)$: We acknowledge that not all true features might be detectable in real data. We introduce $\alpha(x)$, the probability that a true feature at location $x$ is *potentially observable*. Conversely, $1-\alpha(x)$ is the probability that it *vanishes* (is missed entirely, perhaps due to low persistence, noise, or sampling issues). $\alpha(x)$ models feature detectability.
* **Decomposition:** Using $\alpha(x)$, we can think of the original latent process $\mathcal{D}^X$ as being composed of two independent PPPs (this is a standard PPP property called "thinning"):
    * $\mathcal{D}_{XO}$ **(Observable Track):** The subset of latent features that *could* potentially generate an observation in our data. Its intensity is the prior intensity modulated by the probability of being observable: $\color{blue}{\alpha(x) \lambda_{\mathcal{D}_X}(x)}$. This is the "signal" part of our prior that can interact with data (related to the `blue` term in the theorem's denominator).
    * $\mathcal{D}^{XV}$ **(Vanished Track):** The subset of latent features that are fundamentally unobservable. Its intensity is modulated by the probability of vanishing: $\color{red}{(1-\alpha(x)) \lambda_{\mathcal{D}_X}(x)}$. This part of our prior belief cannot be updated by observations (this is the `red` term in the theorem).
* **Consistency:** Note that the intensities sum correctly: $\color{blue}{\alpha(x)\lambda_{\mathcal{D}_X}(x)} + \color{red}{(1-\alpha(x))\lambda_{\mathcal{D}_X}(x)} = \lambda_{\mathcal{D}_X}(x)$, recovering the total prior intensity.
:::

## The Model: Observed PD $\mathcal{D}_Y$ {.smaller}

* An *observed* PD $\mathcal{D}_Y$ is also decomposed into two *independent* components:

::: {.incremental}
* $\mathcal{D}_{YO}$ (**O**bserved from Signal): Points generated *from* the latent observable points $\mathcal{D}_{XO}$.
    * The pair $(\mathcal{D}_{XO}, \mathcal{D}_{YO})$ forms a **Marked PPP**.
    * Connection is via the **stochastic kernel** $\color{purple}{\ell(y|x)}$ (the "likelihood"). It gives the probability density of observing $y \in \mathcal{D}_{YO}$ given a latent point $x \in \mathcal{D}_{XO}$.
* $\mathcal{D}_{YS}$ (**S**purious / Noise): Points arising independently from noise, clutter, or unanticipated geometry.
    * Modeled as an independent PPP with intensity $\color{darkgreen}{\lambda_{\mathcal{D}_{YS}}(y)}$.
:::

::: {.notes}
* Now we turn to Model M3, which describes the structure of the **observed persistence diagrams** $\mathcal{D}_Y$.
* **Context:** Again, this is for our **fixed dimension** $k$. So $\mathcal{D}_Y$ means $\mathcal{D}^k_Y$. Crucially, $\mathcal{D}_Y$ represents the *random process* that generates the actual diagrams $D_{Y^1}, \dots, D_{Y^m}$ which we compute from our data. M3 models the underlying mechanism producing these diagrams.
* **Decomposition:** The model assumes that any observed diagram (a realization of $\mathcal{D}_Y$) is composed of points from two independent sources:
    * 1. $\mathcal{D}_{YO}$ **(Observed from Signal):** These are the points in the observed diagram that correspond to the "true", potentially observable latent features we defined in M2 (the points in $\mathcal{D}_{XO}$).
        * **Marked PPP:** The link between the latent signal $\mathcal{D}_{XO}$ and the observed signal $\mathcal{D}_{YO}$ is modeled as a **Marked Poisson Point Process**. Think of it this way: nature selects a latent feature $x$ to be potentially observable (it's in $\mathcal{D}_{XO}$). Then, the observation process generates a corresponding observed feature $y$ (which becomes a point in $\mathcal{D}_{YO}$) according to some probability distribution. The observed point $y$ is the "mark" associated with the latent point $x$.
        * **Stochastic Kernel** $\color{purple}{\ell(y|x)}$: This kernel defines the probability density of the mark $y$ given the original latent point $x$. This $\color{purple}{\ell(y|x)}$ acts as our **likelihood function** (the `purple` term in the theorem). It models how the process of generating data and computing the PD might shift or perturb a true feature's location from $x$ to $y$.
    * 2. $\mathcal{D}_{YS}$ **(Spurious / Noise):** These are points appearing in the observed diagram that do *not* originate from any true latent feature in $\mathcal{D}_X$. They arise independently from sources like measurement noise, computational artifacts, sampling variations, or other unmodeled aspects.
        * This noise component is modeled as an independent **PPP** governed by its own intensity function $\color{darkgreen}{\lambda_{\mathcal{D}_{YS}}(y)}$. This intensity describes the expected spatial density of purely spurious points appearing near $y$ in an observed diagram. It's our background noise model for dimension $k$ (the `darkgreen` term in the theorem).
* **Interpretation:** According to M3, any specific point $y$ that we see in one of our actual computed diagrams (say, $D_{Y^i}$) could have either been generated by some latent feature $x$ (via $\color{purple}{\ell(y|x)}$) or it could be purely noise (generated according to $\color{darkgreen}{\lambda_{\mathcal{D}_{YS}}(y)}$). The Bayesian theorem will weigh these possibilities.
* *(Refer to Figure 4 if available):* Figure 4 visually summarizes these relationships: $\mathcal{D}_X$ (latent) splits into $\mathcal{D}_{XO}$ (latent signal) and $\mathcal{D}_{XV}$ (latent vanished). $\mathcal{D}_Y$ (observed process) splits into $\mathcal{D}_{YO}$ (observed signal) and $\mathcal{D}_{YS}$ (observed noise). Only $\mathcal{D}_{XO}$ and $\mathcal{D}_{YO}$ are directly linked via the likelihood kernel $\color{purple}{\ell(y|x)}$.
:::

## Bayes' Theorem for PDs

* **Goal:** Find the **posterior intensity** $\lambda_{\mathcal{D}_X | D_{Y^{1:m}}}(x)$ for the latent PD $\mathcal{D}_X$, given $m$ independent observed PDs $D_{Y^1}, \dots, D_{Y^m}$.
* Let $D_{Y^{1:m}} = \cup_{i=1}^m D_{Y^i}$. The posterior intensity is:

::: {.smaller-equation}
$$
\lambda_{\mathcal{D}_X | D_{Y^{1:m}}}(x) = \underbrace{\color{red}{(1 - \alpha(x))\lambda_{\mathcal{D}_X}(x)}}_{\text{Prior Vanished Part}} + \underbrace{\frac{1}{m} \alpha(x) \sum_{i=1}^m \sum_{y \in D_{Y^i}} \frac{ \color{purple}{\ell(y|x)}\color{blue}{ \lambda_{\mathcal{D}_X}(x)}}{\color{darkgreen}{\lambda_{\mathcal{D}_{Y_S}}(y)} + \color{blue}{\int_{\mathbb{W}} \ell(y|u) \alpha(u) \lambda_{\mathcal{D}_X}(u) du}}}_{\text{Update from Observed Points } y} \quad \text{a.s.}
$$
:::

::: {.notes}
This formula gives the posterior intensity $\lambda_{\mathcal{D}_X | D_{Y^{1:m}}}(x)$, which represents our updated belief (as an expected density) about the presence of a latent feature at location $x \in \mathbb{W}$, given the observed data $D_{Y^{1:m}}$. It combines the prior belief with information from the observations. Let's break it down:

* **Overall Structure:** Posterior Intensity = Prior Vanished Part + Update from Observations
* **Term 1: Prior Vanished Part** $\color{red}{(1 - \alpha(x)) \lambda_{D_X(x)}}$
    * $\lambda_{\mathcal{D}_X}(x)$: Prior intensity density at $x$ (from M2).
    * $\alpha(x)$: Probability that a feature at $x$ is potentially observable (from M2).
    * $(1 - \alpha(x))$: Probability that a feature at $x$ is inherently unobservable (vanished, part of $\mathcal{D}^{XV}$).
    * This entire term $(\color{red}{red})$ represents the contribution from the part of the prior belief ($\mathcal{D}^{XV}$) that can **never** be informed by observations. It passes directly to the posterior unchanged.
* **Term 2: Update from Observed Points** $y$ ($\frac{1}{m} \sum ...$ part)
    * This term adjusts the prior belief about *potentially observable* features based on the actual observations $D_{Y^{1:m}}$.
    * $\frac{1}{m}$: Averages the update contribution across the $m$ independent observed diagrams.
    * $\sum_{i=1}^m \sum_{y \in D_{Y^i}}$: Sums the influence of *every observed point* $y$ from *all* $m$ observed diagrams $D_{Y^i}$.
    * **The Fraction:** This core part weights the influence of each observed $y$ on the posterior belief at $x$.
        * **Numerator** $\alpha(x) \ell(y|x) \lambda_{\mathcal{D}_X}(x)$: (Note: $\alpha(x)$ is outside sum in formula, but conceptually linked here)
            * $\color{blue}{\alpha(x) \lambda_{D_X(x)}}$: This is the **prior expected density of observable features specifically at location** $x$. (From M2).
            * $\color{purple}{\ell(y|x)}$: The likelihood kernel (from M3). How well does latent $x$ explain observed $y$? ($\color{purple}{purple}$)
            * The numerator combines the prior belief about observable features at $x$ (via $\alpha(x)$ and $\lambda_{\mathcal{D}_X}(x)$) with the likelihood of $y$ given $x$.
        * **Denominator:** Represents the total expected density of observing a point at location $y$, considering all possibilities:
            * $\color{darkgreen}{\lambda_{D_{Y_S}}(y)}$: Intensity of the spurious/noise process at $y$ (from M3). How likely is $y$ to be just noise? (\color{darkgreen}{darkgreen})
            * $\color{blue}{\text{integral over } W \text{ of } \ell(y|u) \alpha(u) \lambda_{D_X}(u) du}$: The marginal density of observing $y$ generated from *any* potentially observable latent feature $u \in \mathcal{D}^{XO}$. The term \color{blue}{alpha(u)lambda_D_X(u)} inside represents the **prior expected density of observable features at a *generic location u***. ($\color{blue}{ \int}$)
        * The fraction represents the contribution of observation $y$ to updating the belief at $x$, normalized by the total evidence for $y$.
    * **Putting the Update Term Together:** The averaged sum ($\frac{1}{m} \sum \sum ...$, multiplied by $\alpha(x)$) combines the weighted evidence from all observations $y$.
* **What is the Posterior?** The result $\lambda_{\mathcal{D}_X | D_{Y^{1:m}}}(x)$ is the intensity function of a *new* Poisson Point Process. This PPP represents our updated probabilistic model for the latent ("true") persistence diagram $\mathcal{D}_X$ after observing the data $D_{Y^{1:m}}$.
* **Why "almost surely" (a.s.)?**
    * This theorem describes a relationship between *random objects*. The observed diagrams $D_{Y^1}, \dots, D_{Y^m}$ are realizations of random point processes ($\mathcal{D}_Y$). Consequently, the posterior intensity $\lambda_{\mathcal{D}_X | D_{Y^{1:m}}}(x)$ is itself a *random function* because its value depends on this random data.
    * "Almost surely" is a standard term in probability theory meaning the equality holds *with probability 1*. It asserts that for any specific set of observations $D_{Y^{1:m}}$ you might get (excluding zero-probability outcomes), this formula correctly describes the posterior intensity.
:::

## Interpretation & Computation: Gaussian Mixtures {.smaller}

:::{.fragment}
* **Posterior:** $\lambda_{\mathcal{D}_X | D_{Y^{1:m}}}(x)$ blends prior belief (observable & unobservable) with updates from observed $y$ via likelihood $\color{purple}{\ell(y|x)}$ relative to noise.
:::

:::{.fragment}
* **Key Idea:** Likelihood $\color{purple}{\ell(y|x)}$ acts on PD points ($x, y \in \mathbb{W}$), avoiding raw data issues. But posterior calculation (Thm 3.1) can be complex.
:::

::: {.incremental}
* **Solution: Conjugate Priors via Gaussian Mixtures (GMs)**
    * Use **Gaussian Mixtures (GMs)** restricted to $\mathbb{W}$ ($\mathcal{N}^*$) for conjugacy (posterior stays in the same family).
    * **Assume (M2', M3'):** Prior $\lambda_{\mathcal{D}_X}$, Likelihood $\color{purple}{\ell(y|x)}$ (single $\mathcal{N}^*$), Noise $\color{darkgreen}{\lambda_{\mathcal{D}_{YS}}}$ are Gaussian/GM based.
    * **Result (Prop 3.2):** Posterior $\lambda_{\mathcal{D}_X | D_{Y^{1:m}}}(x)$ is **also a GM**.
    * **Benefit:** Parameters update via standard Gaussian rules $\implies$ computationally tractable framework.
:::

::: {.notes}
* **Quick Recap:** The posterior intensity $\lambda_{\mathcal{D}_X | D_{Y^{1:m}}}(x)$ is our updated belief. It merges the prior with evidence from observed points $y$, weighted by the likelihood $\color{purple}{\ell(y|x)}$ and noise levels ($\color{darkgreen}{\lambda_{\mathcal{D}_{YS}}}$ and the integrated signal part).
* **Core Concept:** We work directly with likelihoods $\color{purple}{\ell(y|x)}$ between PD points, which is conceptually powerful and avoids raw data problems. However, the posterior formula isn't always simple to compute.
* **The Fix: Conjugacy with Gaussian Mixtures:** To make this practical, we use a *conjugate prior* approach with Gaussian Mixtures (GMs), restricted to the valid PD wedge ($\mathcal{N}^*$). Conjugacy means the posterior will also be a GM if the prior is.
    * **Setup:** We assume the prior intensity ($\lambda_{\mathcal{D}_X}$), the likelihood kernel ($\color{purple}{\ell(y|x)}$), and the noise intensity ($\color{darkgreen}{\lambda_{\mathcal{D}_{YS}}}$) are all built from these restricted Gaussians/GMs.
    * **Result:** The math works out (Proposition 3.2) – the posterior intensity is guaranteed to be a GM as well.
* **Punchline:** This is great because updating the GM parameters (weights, means, variances) follows standard, computable rules based on Gaussian properties. It makes the Bayesian inference framework efficient.
:::

# Worked Example {#sec-example}

```{r setup-example, include=FALSE}

generate_pd <- function(n_points = 50, noise_variance, seed) {
  set.seed(seed)
  noise_sd <- sqrt(noise_variance)
  theta <- runif(n_points, 0, 2 * pi)
  point_cloud_tbl <- tibble(x = cos(theta), y = sin(theta)) |>
    mutate(
      x = x + rnorm(n(), mean = 0, sd = noise_sd),
      y = y + rnorm(n(), mean = 0, sd = noise_sd)
    )
  point_cloud_matrix <- as.matrix(point_cloud_tbl)
  homology_res <- tryCatch({
      TDAstats::calculate_homology(point_cloud_matrix, dim = 1, threshold = 2)
  }, error = function(e) { NULL })

  if (is.null(homology_res) || nrow(homology_res) == 0) {
    D_Y <- tibble(dimension = integer(), birth = double(), persistence = double())
  } else {
    D_Y <- as_tibble(homology_res) |>
      filter(dimension == 1, is.finite(death)) |>
      mutate(persistence = death - birth) |>
      select(dimension, birth, persistence)
  }
  return(list(point_cloud = point_cloud_tbl, pd = D_Y))
}

calculate_posterior_data <- function(grid_points, observed_pd_data, prior_params, noise_sigma_val, alpha, sigma_y) {
   if (!is.data.frame(observed_pd_data) || nrow(observed_pd_data) == 0) {
    warning("Observed PD is empty or invalid. Returning empty data.")
    return(bind_cols(grid_points, intensity = rep(0.0, nrow(grid_points))))
  }
  if (!all(c("birth", "persistence") %in% names(observed_pd_data))) {
     stop("Observed PD tibble must contain 'birth' and 'persistence' columns.")
   }

  observed_pd_matrix <- observed_pd_data |> select(birth, persistence) |> as.matrix()
  Dy_list <- lapply(seq_len(nrow(observed_pd_matrix)), function(i) observed_pd_matrix[i,])

  # Use fixed noise mean for this example, matching paper's likely setup
  current_noise_params <- list(
      weights = c(1), means = list(c(0.5, 0)), sigmas = c(noise_sigma_val) # sigma = std dev
  )

  intensity_values <- apply(grid_points, 1, function(grid_point_coords) {
    if (grid_point_coords["persistence"] < 0) { return(0.0) }
    x_point <- as.numeric(grid_point_coords)
    intensity_value <- BayesTDA::postIntensityPoisson(
      x = x_point,
      Dy = Dy_list,
      alpha = alpha,
      weight.prior = prior_params$weights,
      mean.prior = prior_params$means,
      sigma.prior = prior_params$sigmas, 
      sigma.y = sigma_y,              
      weights.unexpected = current_noise_params$weights,
      mean.unexpected = current_noise_params$means,
      sigma.unexpected = current_noise_params$sigmas 
    )
    return(ifelse(is.finite(intensity_value), intensity_value, 0.0))
  })
  max_intensity <- max(intensity_values, na.rm = TRUE)
  scaled_intensity <- if (max_intensity > 0) intensity_values / max_intensity else intensity_values

  return(bind_cols(grid_points, intensity = scaled_intensity))
}

plot_intensity <- function(intensity_data, observed_pd_data = NULL, title = NULL, xlim, ylim) {
  p <- ggplot(intensity_data, aes(x = birth, y = persistence)) +
    geom_raster(aes(fill = intensity), interpolate = TRUE) +
    scale_fill_viridis_c(option = "plasma", name = "Scaled\nIntensity", limits = c(0, 1), na.value = "grey80") +
    coord_cartesian(xlim = xlim, ylim = ylim, expand = TRUE) +
    labs(x = "Birth", y = "Persistence", title = title) +
    theme_minimal(base_size = 9) + 
    theme(legend.position = "none", plot.title = element_text(size=rel(1.0)))

  if (!is.null(observed_pd_data) && nrow(observed_pd_data) > 0) {
    p <- p + geom_point(data = observed_pd_data, aes(x = birth, y = persistence),
                        color = "green2", size = 1, shape = 16, stroke=2)
  }
  return(p)
}

calculate_prior_data <- function(grid_points, prior_params) {
  intensity_values <- apply(grid_points, 1, function(point_coords) {
    if (point_coords["persistence"] < 0) { return(0.0) }
    density_value <- tryCatch({
        BayesTDA::Wedge_Gaussian_Mixture(
            x = as.numeric(point_coords),
            weights = prior_params$weights,
            means = prior_params$means,
            sigmas = prior_params$sigmas 
        )
      }, error = function(e) { NA })
    return(ifelse(is.finite(density_value), density_value, 0.0))
  })
  max_intensity <- max(intensity_values, na.rm = TRUE)
  scaled_intensity <- if (max_intensity > 0) intensity_values / max_intensity else intensity_values

  return(bind_cols(grid_points, intensity = scaled_intensity))
}

# --- Global Parameters ---
plot_xlim_main <- c(0, 2.5)
plot_ylim_main <- c(0, 2.5)
grid_res_main <- 30 # Keep low for speed, increase for final plots

prior_inf_params <- list(means = list(c(0.5, 1.2)), sigmas = c(.01), weights = c(1))
prior_weak_params <- list(means = list(c(0.5, 1.2)), sigmas = c(.2), weights = c(1))
prior_uni_params <- list(means = list(c(1, 1)), sigmas = c(1), weights = c(1))
prior_bi_params <- list(means = list(c(0.5, 0.5), c(1.5, 1.5)), sigmas = c(.2, .2), weights = c(0.5, 0.5))

all_priors <- list(
  "Inform" = prior_inf_params,
  "Weak" = prior_weak_params,
  "Uni" = prior_uni_params,
  "Bi" = prior_bi_params
)

prior_names <- list(
  "Inform" = "Informative",
  "Weak" = "Weakly Inf.",
  "Uni" = "Unimodal Uni.",
  "Bi" = "Bimodal Uni."
)

case_params <- tribble(
  ~case_label, ~data_noise_variance, ~seed,
  "Case I",    0.001,              123, 
  "Case II",   0.01,               458, 
  "Case III",  0.1,               789  
)

gen_data <- list()
for(i in 1:nrow(case_params)) {
  params <- case_params[i, ]
  gen_result <- generate_pd(
    n_points = 100,
    noise_variance = params$data_noise_variance,
    seed = params$seed
  )
  gen_data[[params$case_label]] <- gen_result
}

eval_grid <- expand_grid(
    birth = seq(plot_xlim_main[1], plot_xlim_main[2], length.out = grid_res_main),
    persistence = seq(plot_ylim_main[1], plot_ylim_main[2], length.out = grid_res_main)
)

prior_data_list <- map(all_priors, ~calculate_prior_data(eval_grid, .x))
prior_plot_list <- imap(prior_data_list, ~plot_intensity(.x, title = paste("(a)", prior_names[[.y]]), xlim=plot_xlim_main, ylim=plot_ylim_main))
```

## Parameters Summary {.smaller}

```{r param-summary}
param_display <- case_params |>
   mutate(`Data Noise Std Dev (σ_Data)` = sqrt(data_noise_variance)) |>
   select(Case = case_label, `Data Noise Std Dev (σ_Data)`)

kable(param_display, digits=3, caption="Data Generation Parameters") |>
 kable_styling(font_size=30)

```

## Simulated Point Clouds  {.smaller}

```{r sim-pclouds}
plot_single_pc_new <- function(case_lab) {
  pc_data <- gen_data[[case_lab]]$point_cloud
  data_noise_sd <- sqrt(case_params$data_noise_variance[case_params$case_label == case_lab])

  if (is.null(pc_data) || nrow(pc_data) == 0) {
    return(ggplot() + theme_void() + ggtitle(paste0(case_lab, "\n(No data)")))
  }
  ggplot(pc_data, aes(x = x, y = y)) +
    geom_point(alpha = 0.6, size = 1.5) +
    coord_equal(xlim = c(-1.5, 1.5), ylim = c(-1.5, 1.5)) +
    theme_bw(base_size = 10) +
    labs(title = case_lab, subtitle = bquote(sigma[Data] == .(sprintf("%.2f", data_noise_sd)))) +
    theme(plot.subtitle = element_text(hjust = 0.5))
}
p_pc1 <- plot_single_pc_new("Case I")
p_pc2 <- plot_single_pc_new("Case II")
p_pc3 <- plot_single_pc_new("Case III")

(p_pc1 | p_pc2 | p_pc3) + plot_annotation(title = "Generated Point Clouds")
```

## Prior Intensities  {.smaller}

```{r prior-ints}
(prior_plot_list$Inform | prior_plot_list$Weak) / (prior_plot_list$Uni | prior_plot_list$Bi) +
  plot_layout(guides = 'collect') +
  plot_annotation(title = "Prior Intensities") & theme(legend.position = 'right')
```

## Case I Results  {.smaller}

<!-- Low data noise ($\sigma_{Data}=0.01$). All posteriors use $\alpha=1$. -->
<!-- Column (b): Low Likelihood Var ($\sigma_K=0.1$), Model Noise $\sigma_{D_{YS}}\approx 0.316$. -->
<!-- Column (c): High Likelihood Var ($\sigma_K\approx 0.316$), Model Noise $\sigma_{D_{YS}}\approx 0.316$. -->

```{r case1}
alpha_fig6 <- 1.0
noise_sigma_fig6 <- sqrt(0.1) 
sigma_y_scenb <- sqrt(0.01)  
sigma_y_scenc <- sqrt(0.1)  
D_Y_fig6 <- gen_data[["Case I"]]$pd

posterior_data_fig6b <- map(all_priors, ~calculate_posterior_data(eval_grid, D_Y_fig6, .x, noise_sigma_fig6, alpha_fig6, sigma_y_scenb))
posterior_data_fig6c <- map(all_priors, ~calculate_posterior_data(eval_grid, D_Y_fig6, .x, noise_sigma_fig6, alpha_fig6, sigma_y_scenc))

plots_fig6b <- imap(posterior_data_fig6b, ~plot_intensity(.x, D_Y_fig6, title=paste("(b)", prior_names[[.y]]), xlim=plot_xlim_main, ylim=plot_ylim_main))
plots_fig6c <- imap(posterior_data_fig6c, ~plot_intensity(.x, D_Y_fig6, title=paste("(c)", prior_names[[.y]]), xlim=plot_xlim_main, ylim=plot_ylim_main))

p_prior_inf <- prior_plot_list$Inform
p_prior_weak <- prior_plot_list$Weak
p_prior_uni <- prior_plot_list$Uni
p_prior_bi <- prior_plot_list$Bi

# fig6_layout <- "
# ABC
# DEF
# GHI
# JKL
# "

fig6_layout <- "
ADGJ
BEHK
CFIL
"

p_prior_inf <- p_prior_inf + labs(title=paste("(a)", prior_names$Inform))
p_prior_weak <- p_prior_weak + labs(title=paste("(a)", prior_names$Weak))
p_prior_uni <- p_prior_uni + labs(title=paste("(a)", prior_names$Uni))
p_prior_bi <- p_prior_bi + labs(title=paste("(a)", prior_names$Bi))


combined_fig6 <- wrap_plots(
    A = p_prior_inf, B = plots_fig6b$Inform, C = plots_fig6c$Inform,
    D = p_prior_weak, E = plots_fig6b$Weak, F = plots_fig6c$Weak,
    G = p_prior_uni, H = plots_fig6b$Uni, I = plots_fig6c$Uni,
    J = p_prior_bi, K = plots_fig6b$Bi, L = plots_fig6c$Bi,
    design = fig6_layout
) + plot_layout(guides = 'collect') +
    plot_annotation(title = "Case I Results (Low Data Noise)") &
    theme(legend.position = 'right')

print(combined_fig6)
```

:::{.notes}
Column 2 uses a smaller likelihood variance (higher certainty about observation location), and Column 3 uses a larger likelihood variance (lower certainty). The assumed model noise ($\sigma_{D_{YS}}$) stays constant between these columns.
:::

## Case II Results {.smaller}

<!-- Medium data noise ($\sigma_{Data}=0.1$). All posteriors use $\alpha=1$, Likelihood $\sigma_K \approx 0.316$. -->
<!-- Column (b): Low Model Noise ($\sigma_{D_{YS}}\approx 0.316$). -->
<!-- Column (c): High Model Noise ($\sigma_{D_{YS}}=1.0$). -->

```{r case2}
alpha_fig7 <- 1.0
sigma_y_fig7 <- sqrt(0.1)     
noise_sigma_scenb <- sqrt(0.1)
noise_sigma_scenc <- sqrt(1.0)
D_Y_fig7 <- gen_data[["Case II"]]$pd

posterior_data_fig7b <- map(all_priors, ~calculate_posterior_data(eval_grid, D_Y_fig7, .x, noise_sigma_scenb, alpha_fig7, sigma_y_fig7))
posterior_data_fig7c <- map(all_priors, ~calculate_posterior_data(eval_grid, D_Y_fig7, .x, noise_sigma_scenc, alpha_fig7, sigma_y_fig7))

plots_fig7b <- imap(posterior_data_fig7b, ~plot_intensity(.x, D_Y_fig7, title=paste("(b)", prior_names[[.y]]), xlim=plot_xlim_main, ylim=plot_ylim_main))
plots_fig7c <- imap(posterior_data_fig7c, ~plot_intensity(.x, D_Y_fig7, title=paste("(c)", prior_names[[.y]]), xlim=plot_xlim_main, ylim=plot_ylim_main))

fig7_layout <- fig6_layout 

combined_fig7 <- wrap_plots(
    A = p_prior_inf, B = plots_fig7b$Inform, C = plots_fig7c$Inform,
    D = p_prior_weak, E = plots_fig7b$Weak, F = plots_fig7c$Weak,
    G = p_prior_uni, H = plots_fig7b$Uni, I = plots_fig7c$Uni,
    J = p_prior_bi, K = plots_fig7b$Bi, L = plots_fig7c$Bi,
    design = fig7_layout
) + plot_layout(guides = 'collect') +
    plot_annotation(title = "Case II Results (Med Data Noise)") &
    theme(legend.position = 'right')

print(combined_fig7)
```

:::{.notes}
Column 2 uses a smaller model noise variance, and Column 3 uses a larger model noise variance. The assumed likelihood variance ($\sigma_K$) stays constant between these columns.
:::

## Case III Results {.smaller}
<!-- High data noise ($\sigma_{Data}=0.3$). All posteriors use $\alpha=1$, Model Noise $\sigma_{D_{YS}}\approx 0.316$. -->
<!-- Column (b): Low Likelihood Var ($\sigma_K=0.1$). -->
<!-- Column (c): High Likelihood Var ($\sigma_K\approx 0.316$). -->

```{r case3}
alpha_fig8 <- 1.0
noise_sigma_fig8 <- sqrt(0.1)
sigma_y_scenb <- sqrt(0.01) 
sigma_y_scenc <- sqrt(0.1) 
D_Y_fig8 <- gen_data[["Case III"]]$pd

posterior_data_fig8b <- map(all_priors, ~calculate_posterior_data(eval_grid, D_Y_fig8, .x, noise_sigma_fig8, alpha_fig8, sigma_y_scenb))
posterior_data_fig8c <- map(all_priors, ~calculate_posterior_data(eval_grid, D_Y_fig8, .x, noise_sigma_fig8, alpha_fig8, sigma_y_scenc))

plots_fig8b <- imap(posterior_data_fig8b, ~plot_intensity(.x, D_Y_fig8, title=paste("(b)", prior_names[[.y]]), xlim=plot_xlim_main, ylim=plot_ylim_main))
plots_fig8c <- imap(posterior_data_fig8c, ~plot_intensity(.x, D_Y_fig8, title=paste("(c)", prior_names[[.y]]), xlim=plot_xlim_main, ylim=plot_ylim_main))

fig8_layout <- fig6_layout 

combined_fig8 <- wrap_plots(
    A = p_prior_inf, B = plots_fig8b$Inform, C = plots_fig8c$Inform,
    D = p_prior_weak, E = plots_fig8b$Weak, F = plots_fig8c$Weak,
    G = p_prior_uni, H = plots_fig8b$Uni, I = plots_fig8c$Uni,
    J = p_prior_bi, K = plots_fig8b$Bi, L = plots_fig8c$Bi,
    design = fig8_layout
) + plot_layout(guides = 'collect') +
    plot_annotation(title = "Case III Results (High Data Noise)") &
    theme(legend.position = 'right')

print(combined_fig8)
```

:::{.notes}
Column 2 uses a smaller likelihood variance (higher certainty about observation location), and Column 3 uses a larger likelihood variance (lower certainty). The assumed model noise ($\sigma_{D_{YS}}$) stays constant between these columns.
:::

## Case IV Results  {.smaller}
<!-- Effect of lower detectability ($\alpha=0.5$) using parameters from previous figures. -->
<!-- Column 1: Case I data, params from Fig 6c ($\sigma_K\approx 0.316, \sigma_{D_{YS}}\approx 0.316$). -->
<!-- Column 2: Case II data, params from Fig 7c ($\sigma_K\approx 0.316, \sigma_{D_{YS}}=1.0$). -->
<!-- Column 3: Case III data, params from Fig 8b ($\sigma_K=0.1, \sigma_{D_{YS}}\approx 0.316$). -->

```{r case4}
alpha_fig9 <- 0.5 

D_Y_col1 <- gen_data[["Case I"]]$pd
sigma_y_col1 <- sqrt(0.1)
noise_sigma_col1 <- sqrt(0.1)

D_Y_col2 <- gen_data[["Case II"]]$pd
sigma_y_col2 <- sqrt(0.1)
noise_sigma_col2 <- sqrt(1.0)

D_Y_col3 <- gen_data[["Case III"]]$pd
sigma_y_col3 <- sqrt(0.01)
noise_sigma_col3 <- sqrt(0.1)

posterior_data_col1 <- map(all_priors, ~calculate_posterior_data(eval_grid, D_Y_col1, .x, noise_sigma_col1, alpha_fig9, sigma_y_col1))
posterior_data_col2 <- map(all_priors, ~calculate_posterior_data(eval_grid, D_Y_col2, .x, noise_sigma_col2, alpha_fig9, sigma_y_col2))
posterior_data_col3 <- map(all_priors, ~calculate_posterior_data(eval_grid, D_Y_col3, .x, noise_sigma_col3, alpha_fig9, sigma_y_col3))

plots_col1 <- imap(posterior_data_col1, ~plot_intensity(.x, D_Y_col1, title=paste(prior_names[[.y]]), xlim=plot_xlim_main, ylim=plot_ylim_main))
plots_col2 <- imap(posterior_data_col2, ~plot_intensity(.x, D_Y_col2, title=paste(prior_names[[.y]]), xlim=plot_xlim_main, ylim=plot_ylim_main))
plots_col3 <- imap(posterior_data_col3, ~plot_intensity(.x, D_Y_col3, title=paste(prior_names[[.y]]), xlim=plot_xlim_main, ylim=plot_ylim_main))

fig9_layout <- fig6_layout

p_prior_inf <- p_prior_inf + labs(title=prior_names$Inform)
p_prior_weak <- p_prior_weak + labs(title=prior_names$Weak)
p_prior_uni <- p_prior_uni + labs(title=prior_names$Uni)
p_prior_bi <- p_prior_bi + labs(title=prior_names$Bi)

combined_fig9 <- wrap_plots(
    A = plots_col1$Inform, B = plots_col2$Inform, C = plots_col3$Inform,
    D = plots_col1$Weak, E = plots_col2$Weak, F = plots_col3$Weak,
    G = plots_col1$Uni, H = plots_col2$Uni, I = plots_col3$Uni,
    J = plots_col1$Bi, K = plots_col2$Bi, L = plots_col3$Bi,
    design = fig9_layout
) + plot_layout(guides = 'collect') +
    plot_annotation(title = "Case IV Results (Alpha = 0.5)") &
    theme(legend.position = 'right')

print(combined_fig9)
```

<!-- ## Conclusions {.smaller} -->
<!-- Bayesian inference successfully combines prior knowledge and observed PD data. -->
<!-- Posterior intensity reflects features present in data, modulated by prior beliefs and model uncertainty parameters ($\sigma_K, \sigma_{D_{YS}}, \alpha$). -->
<!-- Low-noise data & informative priors lead to more concentrated posteriors. -->
<!-- High noise/uncertainty (in data or model parameters) or weak priors lead to more diffuse posteriors, where prior influence is more visible. -->
<!-- Lower detectability ($\alpha$) increases posterior uncertainty and reliance on the prior. -->
<!-- The framework allows systematic study of how data quality and model assumptions impact topological feature inference. -->


# Classification {#sec-classification}

<!-- ## Atom Probe Tomography Data -->

## Real-World Example: Classifying Crystal Lattices {.smaller}

:::{.fragment}
**Problem:**
:::
:::{.incremental}
* Classify **Body-Centered Cubic (BCC)** vs. **Face-Centered Cubic (FCC)** crystal structures in high-entropy alloys.
* Data is from **Atom Probe Tomography (APT)**: inherently noisy and sparse (~65% missing data).
* Existing methods struggle with combined noise/sparsity or require prior knowledge of global structure.
:::


:::{.fragment}
**Approach using Persistent Homology (PH):**
:::
:::{.incremental}
* Represent spatial atom configurations as point clouds.
* Compute **Rips filtrations** and extract **1-dimensional Persistence Diagrams (PDs)**.
* **Idea:** Topological features (connectedness, holes/voids) captured by PDs differ between BCC (center atom) and FCC (center void, face atoms).
    * *(See Figure 10 in paper for example PDs from each class)*.
:::


<!-- ## Real-World Example: Classifying Crystal Lattices {.smaller} -->

<!-- **Bayesian Classification Framework:** -->

<!-- * Model PDs as realizations of Poisson Point Processes ($\mathcal{D}$). -->
<!-- * Use training data ($T_{BCC}$, $T_{FCC}$) and priors ($\lambda_{BCC}$, $\lambda_{FCC}$) to compute posterior probabilities $p(D | T_{BCC})$ and $p(D | T_{FCC})$ for a new diagram $D$. -->

## Bayesian Classification: Method & Results {.smaller}

**Experiment Setup:**

:::{.incremental}
* **Data:** 200 BCC PDs, 200 FCC PDs.
* **Priors Tested :**
    * *Prior-1:* Class-specific (means from K-means on subset of data).
    * *Prior-2:* Common "flat" prior (mean=(1,1), $\sigma= \sqrt{20}$, high variance).
* **Likelihood/Noise Params:**
    * Likelihood kernel std dev: $\sigma_{K} = 0.1$ (referred to as $\sigma_{D_{YO}}$ in text).
    * Unexpected noise intensity: $\lambda_{D_{YS}}(x) = 5 \mathcal{N}^*(x; (0, 0), \sigma=\sqrt{0.2})$. (High weight=5 reflects noise features are rare).
* **Evaluation:** 10-fold Cross-Validation, Area Under ROC Curve (AUC), Bootstrapped 95% CIs.
:::

## Bayesian Classification: Method & Results {.smaller}

**Results (AUC):**

```{r apt-results-table, echo=FALSE, message=FALSE, warning=FALSE}
results_tbl <- tribble(
  ~Prior, ~`Mean AUC`, ~`5th Percentile`, ~`95th Percentile`,
  "Prior-1 (Class-Specific)", 0.941, 0.931, 0.958,
  "Prior-2 (Common Flat)",    0.940, 0.928, 0.951
)

kable(results_tbl, digits = 3, caption = "10-Fold CV AUC Summary (Bootstrapped)") |>
  kable_styling(font_size = "1em") 
```

:::{.fragment}
**Conclusion:**

The Bayesian framework achieves near-perfect classification accuracy (Mean AUC ≈ 0.94).
Results are robust to the choice of prior (informative vs. flat).
Demonstrates the framework's capability for machine learning tasks on PDs for challenging real-world data.
:::

:::{.notes}
- Receiving Operating Characteristic
- Binary classification over all decision thresholds
- True positive rate - correctly identify positive outcome
- False positive rate - negatives classified as positive
:::

# References {#sec-bib}

## References

::: {#refs .smaller}
:::

