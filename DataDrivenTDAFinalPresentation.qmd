---
title: "The Shape of Digits"
subtitle: "A Bayesian Topological Data Analytic Approach to Classification of Handwritten Digits"
authors:
  - name: Thomas Reinke
    affiliation: 
      - name: Baylor University
        department: Statistical Science
        # city: Waco
        # state: TX
        # country: US
        url: https://www.baylor.edu
    # email: thomas_reinke1@baylor.edu
  - name: Theophilus A. Bediako
    affiliation: 
      - name: Baylor University
        department: Statistical Science
        # city: Waco
        # state: TX
        # country: US
        url: https://www.baylor.edu
  - name: Daniel Lim
    affiliation: 
      - name: Baylor University
        department: Statistical Science
        # city: Waco
        # state: TX
        # country: US
        url: https://www.baylor.edu
date: today
date-format: "MMMM D, YYYY"
format: 
  revealjs:
    theme: 
      - quarto-assets/baylor-theme.scss
    smaller: false
    scrollable: false
    show-slide-number: all
    toc: false
    toc-depth: 1
    preview-links: true
    slide-number: c/t
    multiplex: false
    embed-resources: true
    auto-animate: true
    #footer: "Thomas Reinke"
bibliography: references.bibtex
lightbox:
  match: auto
  effect: fade
  desc-position: bottom
  loop: true
logo: "quarto-assets/baylor.png"
license: "CC BY-NC"
copyright: 
  holder: Thomas Reinke
  year: 2025
editor: 
  markdown: 
    wrap: 72
fig-width: 15
---

```{r, setup}
#| include: false
#| message: false
library(quarto)
library(knitr)
library(tidyverse)
library(conflicted)
library(janitor)
library(ggtda)
# library(TDAvis)
library(patchwork)
library(gganimate)
library(ggforce)
library(simplextree) 
library(gifski)
library(magick)  
library(ripserr)
library(reshape2)
# remotes::install_github("maroulaslab/BayesTDA") Use this if package ‘BayesTDA’ is not available for this version of R
library(BayesTDA)
library(TDAstats)
library(mvtnorm)
library(kableExtra)
library(plotly)
library(DiagrammeR)
library(transport)
library(TDA)
library(RColorBrewer)
library(Rtsne)
library(keras)
conflicted::conflict_prefer("filter", "dplyr")
conflicted::conflict_prefer("select", "dplyr")
conflicted::conflicts_prefer(ggtda::geom_simplicial_complex)
conflicted::conflicts_prefer(plotly::layout)
knitr::opts_chunk$set(
  comment = "#>",
  message = FALSE,
  warning = FALSE,
  cache = FALSE,
  echo = FALSE,
  tidy.opts = list(width.cutoff = 100),
  tidy = FALSE,
  fig.align = "center"
)
ggplot2::theme_set(ggplot2::theme_minimal())
ggplot2::theme_update(panel.grid.minor = ggplot2::element_blank())

#------------------------------------------------------------#
```

::: {.content-hidden}
$$
{{< include quarto-assets/_macros.tex >}}
$$
:::


```{r load_data}
#--------------------------------------------------------
#----------Load & Preprocess Data------------------------
#--------------------------------------------------------
mnist <- readRDS(file = "mnist_dataset")

train <- mnist$train
test <- mnist$test

train_images <- train$images
train_labels <- as.factor(train$labels)  

test_images <- test$images  
test_labels <- as.factor(test$labels)

train_images <- train_images / 255
test_images <- test_images / 255

train_images_list <- lapply(1:nrow(train_images), function(i) {
  matrix(train_images[i, ], nrow = 28) |> t()
})

test_images_list <- lapply(1:nrow(test_images), function(i) {
  matrix(test_images[i, ], nrow = 28) |> t()
})

plot_digit <- \(image_list = train_images_list, image_index = NULL, image_df = NULL, melted = FALSE){
  if(!melted){
    image_df <- melt(image_list[image_index])
    colnames(image_df) <- c("y", "x", "value")
  }
  ggplot(image_df, aes(x = x, y = y, fill = value)) + 
    geom_raster() +
  scale_fill_gradient(low = "white", high = "black") +
  scale_y_reverse() + 
  coord_equal() +
  theme_void() + 
  theme(legend.position = "none") 
}

# plot_digit(image_index = 8)
# paste0("Label: ", train$labels[8])

binarize_images <- function(images_list, threshold = 0.5) {
  lapply(images_list, function(mat) {
    ifelse(mat < threshold, 0, 1)
  })
}

train_images_binarized <- binarize_images(train_images_list)
test_images_binarized <- binarize_images(test_images_list)

# plot_digit(image_index = 8) + plot_digit(train_images_binarized, image_index = 8)

#------------------------------------------------------------#
```

# Contents

1. [MNIST EDA](#sec-EDA)
1. [Tradiotional ML](#sec-MLMethod)
1. [Proposed Methodology](#sec-PropMethod)
1. [TDA + ML](#sec-TDAML)
1. [Results/Future Work](#sec-Results)
1. [References](#sec-References)


# Exploratory Data Analysis {#sec-EDA}


## Distribution of training labels

```{r dist_labels, fig.dim=c(10,6)}
#--------------------------------------------------------
#----------Distribution of training labels---------------
#--------------------------------------------------------
#first run the qmd file
train_df <- train$images |> as.data.frame()
train_df$labels <- train$labels |> as.factor()

ggplot(train_df, aes(labels, fill = labels))+
  geom_bar()+
  labs(fill = "digit")

```

:::{.notes}
- Around 6000 digits in each class
- No class imbalance
:::

## Pixel Intensity

```{r pixel_intensity, fig.dim=c(10,6)}
#----------------------------------------------------------
#-----------Pixel intensity representation-----------------
#----------------------------------------------------------
train_df_long <- train_df |>
  pivot_longer(-labels, names_to  = "covariate", values_to = "intensity")
ggplot(train_df_long) +
  geom_histogram(
    aes(intensity),
    bins = 30,
    fill = "#2C7FB8",
    color = "white",
    alpha = 0.8
  ) 
# ggplot(train_df_long) +
#   geom_histogram(
#     aes(intensity),
#     bins = 30,
#     fill = "#2C7FB8",
#     color = "white",
#     alpha = 0.8
#   ) +
#   scale_y_log10() +
#   labs(y = "Count (log scale)")

```

## Training Data tSNE Visualization

```{r tnse, fig.dim=c(10,6)}
#------------------------------------------------------------
#----------------distribution of mnist training--------------
#------------------------------------------------------------
#normalized pixel values and apply t-SNE
# tsne_results <- Rtsne(
#   train_images,
#   dim = 2,
#   perplexity = 30,
#   max_iter = 1000
# )
# df_tsne <- tibble(Dim1 = tsne_results$Y[, 1],
#                   Dim2 = tsne_results$Y[, 2],
#                   digit = train_df$labels)
#
# saveRDS(df_tsne, "df_tsne")
df_tsne <- readRDS("df_tsne")
ggplot(df_tsne, aes(Dim1, Dim2, color = digit)) +
  geom_point(alpha = .5)
```

:::{.notes}
- We adopt t-distributed Stochastic Neighbor Embedding(t-SNE) to represent the data in 2D.See [T-SNE Exploration by TusVasMit](https://rpubs.com/TusVasMit/T-SNEExploration) for more details.
:::

# Traditional ML

## Neural networks

:::{.fragment}
Feedforward neural network with structure:
:::

:::{.incremental}
- Input layer: Consists of neurons that receives the input data each neuron in the input layer represents a feature of the input data
- Hidden layer: One or more hidden layers placed between the input and output layers, responsible for capturing complex patterns
- Output layer: Final output of the network; Number of neurons represents the number of digits
:::

## NN with regularization

:::{.incremental}
- Depending on model, \# network weights > size of training data
    - This leads to overfitting
- We considered two approaches to overfitting:
  - Dropout learning: Like RF, randomly removes fraction of units in layer during model fitting
  - Regularization: Impose penalties on parameters like lasso, ridge, etc.
:::


## Specific NN models considered

:::{.fragment}
- NN with dropout regularization
:::
:::{.fragment}
- NN with ridge regularization
:::
:::{.fragment}
- NN with lasso regularization 
:::

## Multinomial logistic regression

:::{.incremental}
- Multinomial logistic regression equivalently represented by NN with no hidden layers
- Output layer with softmax
    - $f_m(X) = Pr(Y = m | X) = \frac{e^{Z_m}}{\sum_\limits{l \in K}e^{Z_l}}$
:::

:::{.notes}
- $m$ is the class label
- $Z_m$ is the output of the model for class $m$
- $X$ is the input data
- $K$ is the set of all class labels
- $l$ is a class label in $K$
:::

## NN Fitting

:::{.incremental}
- Train the network for 30 epochs with a default batch size of 32
    - SGD updates weights for each batch
- Images are presented in batches of 32, and SGD updates weights after each batch
- Each epoch processes all 60,000 training images
<!-- - After 30 epochs, the network’s error (loss) is visualized on the left graph -->
<!-- - Accuracy shown on the right graph -->
- Classification correct if largest output value matches target label
:::

:::{.notes}
- SGD is an optimization algorithm used to minimize the loss function by updating the model's weights based on the gradients of the loss with respect to the weights
:::

<!-- Let’s train the network for 30 epochs, using a default batch size of 32. This means that images from the training set will be presented to the network in batches of 32 at a time, and for each batch, the SGD algorithm will update the network’s weights by an appropriate amount. Then another batch of 32 images will be presented, and so on, until all 60,000 training images in the dataset have been processed, which constitutes one epoch of training. This entire cycle will be repeated for 30 epochs. As training proceeds, the network’s error (loss) on both the training and testing/validation sets will be shown on the left graph, and the accuracy on each of these sets will be shown on the right graph. The accuracy is simply the fraction of input images that the network classifies correctly. A classification is considered correct if the largest output value on the output layer corresponds to the target classification -->

# Proposed Methodology {#sec-PropMethod}

## TDA Workflow

```{tikz}
%| echo: false

\usetikzlibrary{positioning, arrows.meta}

\begin{tikzpicture}[
    node distance = 1.2cm and 2cm,
    auto,
    every node/.style={
        draw,
        thick,
        rounded corners,
        align=center,
        minimum height=1.2cm
    },
    arrow/.style={->, >=Stealth}
]

% Define nodes for the flowchart
\node (train) {Train Data \\ 60,000 \\ 28x28};
\node (test) [below=2.5cm of train] {Test Data \\ 10,000 \\ 28x28};

\node (PDdots) [right=2cm of train] {$\vdots$};
\node (OPD0) [above=.5cm of PDdots] {Observed PDs for 0};
\node (priors) [above=2cm of train] {Prior};
\node (OPD9) [below=.5cm of PDdots] {Observed PDs for 9};

\node (update) [right=1.5cm of PDdots, draw=none, scale=2] {$\odot$};
\node (priordots) [right=2cm of update] {$\vdots$};
\node (prior0) [above=.5cm of priordots] {Uniformitive Prior for 0};
\node (prior9) [below=0.5cm of priordots] {Uniformitive Prior for 9};

\node (equals) [right=2cm of priordots, draw=none, scale=2] {=};

\node (postdots) [right=1.25cm of equals] {$\vdots$};
\node (post0) [above=.5cm of postdots] {Posterior for 0};
\node (post9) [below=.5cm of postdots] {Posterior for 9};

\node (testDist) [right=2.5cm of postdots] {10 Disatnces/PD};
\node (testPD) [below=2.5cm of testDist] {Test PDs};

\node (classify) [right=.5cm of testDist] {Classify as $\mathrm{argmin}$ Distances};

% Draw edges between nodes

\draw [arrow] (train) -- (OPD0);
\draw [arrow] (train) -- (PDdots);
\draw [arrow] (train) -- (OPD9);
\draw [arrow] (priors) -| (prior0);

%\draw [arrow, bend left=45] (OPD0) to (post0);
\draw [arrow] (OPD0) -- ++(0,1cm) -- ++(2cm,0) -| (post0);
\draw [arrow] (OPD9) -- ++(0,-1cm) -- ++(2cm,0) -| (post9);
\draw [arrow] (prior0) -- (post0);
\draw [arrow] (prior9) -- (post9);

\draw [arrow] (post0) -- (testDist);
\draw [arrow] (postdots) -- (testDist);
\draw [arrow] (post9) -- (testDist);
\draw [arrow] (testPD) -- (testDist);
\draw [arrow] (test) -- (testPD);

\draw [arrow] (testDist) -- (classify);
%\draw [arrow, bend left] (posterior) to node [pos=0.5, above] {EB} (proj);

\end{tikzpicture}
```

# TDA + ML {#sec-TDAML}

# Analysis {#sec-Analysis}

## ML Analysis

## Proposed Method Analysis

## TDA + ML Analysis


# Results/Future Work {#sec-Results}

## ML Results

```{r ml_results}
#run the Mnist ML method code first, and take results from there
# ml_results <- tibble(
#   method = c("multinomial", "dropout nn", "ridge nn", "lasso no"),
#   accuracy = c(mlogit_acc, nn_dropout_accu, nn_ridge_accu, nn_lasso_accu)
# )
# saveRDS(ml_results, "ml_results")

ml_results <- readRDS("ml_results") #still a tibble so easy to edit

ml_results |> 
  kable(digits = 4)
```

## Propsed Results

## ML + TDA Results

# References {#sec-References}

## References

::: {#refs .smaller}
:::
