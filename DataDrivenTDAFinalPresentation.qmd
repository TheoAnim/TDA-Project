---
title: "The Shape of Digits"
subtitle: "A Bayesian Topological Data Analytic Approach to Classification of Handwritten Digits"
authors:
  - name: Thomas Reinke
    affiliation: 
      - name: Baylor University
        department: Statistical Science
        # city: Waco
        # state: TX
        # country: US
        url: https://www.baylor.edu
    # email: thomas_reinke1@baylor.edu
  - name: Theophilus A. Bediako
    affiliation: 
      - name: Baylor University
        department: Statistical Science
        # city: Waco
        # state: TX
        # country: US
        url: https://www.baylor.edu
  - name: Daniel Lim
    affiliation: 
      - name: Baylor University
        department: Statistical Science
        # city: Waco
        # state: TX
        # country: US
        url: https://www.baylor.edu
date: today
date-format: "MMMM D, YYYY"
format: 
  revealjs:
    theme: 
      - quarto-assets/baylor-theme.scss
    smaller: false
    scrollable: false
    show-slide-number: all
    toc: false
    toc-depth: 1
    preview-links: true
    slide-number: c/t
    multiplex: false
    embed-resources: true
    auto-animate: true
    #footer: "Thomas Reinke"
bibliography: references.bibtex
lightbox:
  match: auto
  effect: fade
  desc-position: bottom
  loop: true
logo: "quarto-assets/baylor.png"
license: "CC BY-NC"
copyright: 
  holder: Thomas Reinke
  year: 2025
editor: 
  markdown: 
    wrap: 72
fig-width: 15
---

```{r, setup}
#| include: false
#| message: false
library(quarto)
library(knitr)
library(tidyverse)
library(conflicted)
library(janitor)
library(ggtda)
# library(TDAvis)
library(patchwork)
library(gganimate)
library(ggforce)
library(simplextree) 
library(gifski)
library(magick)  
library(ripserr)
library(reshape2)
# remotes::install_github("maroulaslab/BayesTDA") Use this if package ‘BayesTDA’ is not available for this version of R
library(BayesTDA)
library(TDAstats)
library(mvtnorm)
library(kableExtra)
library(plotly)
library(DiagrammeR)
library(transport)
library(TDA)
library(RColorBrewer)
library(Rtsne)
library(keras)
library(furrr)
library(yardstick)
conflicted::conflict_prefer("filter", "dplyr")
conflicted::conflict_prefer("select", "dplyr")
conflicted::conflicts_prefer(ggtda::geom_simplicial_complex)
conflicted::conflicts_prefer(plotly::layout)
knitr::opts_chunk$set(
  comment = "#>",
  message = FALSE,
  warning = FALSE,
  cache = FALSE,
  echo = FALSE,
  tidy.opts = list(width.cutoff = 100),
  tidy = FALSE,
  fig.align = "center"
)
ggplot2::theme_set(ggplot2::theme_minimal())
ggplot2::theme_update(panel.grid.minor = ggplot2::element_blank())

#------------------------------------------------------------#
```

::: {.content-hidden}
$$
{{< include quarto-assets/_macros.tex >}}
$$
:::


```{r load_data}
#--------------------------------------------------------
#----------Load & Preprocess Data------------------------
#--------------------------------------------------------
mnist <- readRDS(file = "mnist_dataset")

train <- mnist$train
test <- mnist$test

train_images <- train$images
train_labels <- as.factor(train$labels)  

test_images <- test$images  
test_labels <- as.factor(test$labels)

train_images <- train_images / 255
test_images <- test_images / 255

train_images_list <- lapply(1:nrow(train_images), function(i) {
  matrix(train_images[i, ], nrow = 28) |> t()
})

test_images_list <- lapply(1:nrow(test_images), function(i) {
  matrix(test_images[i, ], nrow = 28) |> t()
})

plot_digit <- \(image_list = train_images_list, image_index = NULL, image_df = NULL, melted = FALSE){
  if(!melted){
    image_df <- melt(image_list[image_index])
    colnames(image_df) <- c("y", "x", "value")
  }
  ggplot(image_df, aes(x = x, y = y, fill = value)) + 
    geom_raster() +
  scale_fill_gradient(low = "white", high = "black") +
  scale_y_reverse() + 
  coord_equal() +
  theme_void() + 
  theme(legend.position = "none") 
}

# plot_digit(image_index = 8)
# paste0("Label: ", train$labels[8])

binarize_images <- function(images_list, threshold = 0.5) {
  lapply(images_list, function(mat) {
    ifelse(mat < threshold, 0, 1)
  })
}

train_images_binarized <- binarize_images(train_images_list)
test_images_binarized <- binarize_images(test_images_list)

# plot_digit(image_index = 8) + plot_digit(train_images_binarized, image_index = 8)

#------------------------------------------------------------#
```

# Contents

1. [MNIST EDA](#sec-EDA)
1. [Tradiotional ML](#sec-MLMethod)
1. [Proposed Methodology](#sec-PropMethod)
1. [Analysis](#sec-Analysis)
1. [TDA + ML](#sec-TDAML)
1. [Results/Future Work](#sec-Results)
1. [References](#sec-References)


# Exploratory Data Analysis {#sec-EDA}


## Distribution of training labels

```{r dist_labels, fig.dim=c(10,6)}
#--------------------------------------------------------
#----------Distribution of training labels---------------
#--------------------------------------------------------
#first run the qmd file
train_df <- train$images |> as.data.frame()
train_df$labels <- train$labels |> as.factor()

ggplot(train_df, aes(labels, fill = labels))+
  geom_bar()+
  labs(fill = "digit")

```

:::{.notes}
- Around 6000 digits in each class
- No class imbalance
:::

## Pixel Intensity

```{r pixel_intensity, fig.dim=c(10,6)}
#----------------------------------------------------------
#-----------Pixel intensity representation-----------------
#----------------------------------------------------------
# train_df_long <- train_df |>
#   pivot_longer(-labels, names_to  = "covariate", values_to = "intensity")
# saveRDS(train_df_long, "train_df_long.rds")
train_df_long <- readRDS("train_df_long.rds")

ggplot(train_df_long) +
  geom_histogram(
    aes(intensity),
    bins = 30,
    fill = "#2C7FB8",
    color = "white",
    alpha = 0.8
  ) 
# ggplot(train_df_long) +
#   geom_histogram(
#     aes(intensity),
#     bins = 30,
#     fill = "#2C7FB8",
#     color = "white",
#     alpha = 0.8
#   ) +
#   scale_y_log10() +
#   labs(y = "Count (log scale)")

```

## Training Data tSNE Visualization

```{r tnse, fig.dim=c(10,6)}
#------------------------------------------------------------
#----------------distribution of mnist training--------------
#------------------------------------------------------------
#normalized pixel values and apply t-SNE
# tsne_results <- Rtsne(
#   train_images,
#   dim = 2,
#   perplexity = 30,
#   max_iter = 1000
# )
# df_tsne <- tibble(Dim1 = tsne_results$Y[, 1],
#                   Dim2 = tsne_results$Y[, 2],
#                   digit = train_df$labels)
#
# saveRDS(df_tsne, "df_tsne")
df_tsne <- readRDS("df_tsne")
ggplot(df_tsne, aes(Dim1, Dim2, color = digit)) +
  geom_point(alpha = .5)
```

:::{.notes}
- We adopt t-distributed Stochastic Neighbor Embedding(t-SNE) to represent the data in 2D.See [T-SNE Exploration by TusVasMit](https://rpubs.com/TusVasMit/T-SNEExploration) for more details.
:::

# Traditional ML

## Neural networks

:::{.fragment}
Feedforward neural network with structure:
:::

:::{.incremental}
- Input layer: Consists of neurons that receives the input data each neuron in the input layer represents a feature of the input data
- Hidden layer: One or more hidden layers placed between the input and output layers, responsible for capturing complex patterns
- Output layer: Final output of the network; Number of neurons represents the number of digits
:::

## NN with regularization

:::{.incremental}
- Depending on model, \# network weights > size of training data
    - This leads to overfitting
- We considered two approaches to overfitting:
  - Dropout learning: Like RF, randomly removes fraction of units in layer during model fitting
  - Regularization: Impose penalties on parameters like lasso, ridge, etc.
:::


## Specific NN models considered

:::{.fragment}
- NN with dropout regularization
:::
:::{.fragment}
- NN with ridge regularization
:::
:::{.fragment}
- NN with lasso regularization 
:::

## Multinomial logistic regression

:::{.incremental}
- Multinomial logistic regression equivalently represented by NN with no hidden layers
- Output layer with softmax
    - $f_m(X) = Pr(Y = m | X) = \frac{e^{Z_m}}{\sum_\limits{l \in K}e^{Z_l}}$
:::

:::{.notes}
- $m$ is the class label
- $Z_m$ is the output of the model for class $m$
- $X$ is the input data
- $K$ is the set of all class labels
- $l$ is a class label in $K$
:::

## NN Fitting

:::{.incremental}
- Train the network for 30 epochs with a default batch size of 32
    - SGD updates weights for each batch
- Images are presented in batches of 32, and SGD updates weights after each batch
- Each epoch processes all 60,000 training images
<!-- - After 30 epochs, the network’s error (loss) is visualized on the left graph -->
<!-- - Accuracy shown on the right graph -->
- Classification correct if largest output value matches target label
:::

:::{.notes}
- SGD is an optimization algorithm used to minimize the loss function by updating the model's weights based on the gradients of the loss with respect to the weights
- This process iteratively adjusts the weights to improve the model's performance on the training data
:::

<!-- Let’s train the network for 30 epochs, using a default batch size of 32. This means that images from the training set will be presented to the network in batches of 32 at a time, and for each batch, the SGD algorithm will update the network’s weights by an appropriate amount. Then another batch of 32 images will be presented, and so on, until all 60,000 training images in the dataset have been processed, which constitutes one epoch of training. This entire cycle will be repeated for 30 epochs. As training proceeds, the network’s error (loss) on both the training and testing/validation sets will be shown on the left graph, and the accuracy on each of these sets will be shown on the right graph. The accuracy is simply the fraction of input images that the network classifies correctly. A classification is considered correct if the largest output value on the output layer corresponds to the target classification -->

# Proposed Methodology {#sec-PropMethod}

## TDA Workflow

```{tikz}
%| echo: false

\usetikzlibrary{
    positioning, 
    arrows.meta, 
    shapes.geometric, 
    fit, 
    calc
}

\begin{tikzpicture}[
    % Adjusted node distances for better spacing
    node distance = 1.2cm and 2cm,
    every node/.style={
        draw, 
        thick, 
        rounded corners, 
        align=center, 
        minimum height=1.3cm,
        font=\sffamily
    },
    data/.style={fill=green!20, text width=3cm},
    prior/.style={fill=yellow!30, text width=4cm},
    posterior/.style={fill=blue!20, text width=4cm},
    result/.style={fill=red!20, text width=3.5cm},
    process/.style={text width=4cm},
    arrow/.style={->, >=Stealth, thick},
    connector/.style={draw=none, font=\sffamily\Huge},
    % A dedicated style for labels on arrows (edges)
    edge_label/.style={draw=none, midway, fill=none, font=\sffamily}
]

% == Column 1 & 2: Data and PD Calculation ==
% Position nodes in the first two columns
\node[data] (train) {Train Data \\ (60,000 images)};
\node[process, right=of train] (calc_pd_train) {Calculate Train PDs \\ (for dim0 \& dim1)};

% Increased vertical distance for a clearer separation of train/test paths
\node[data, below=3.75cm of train] (test) {Test Data \\ (10,000 images)};
\node[process, right=of test] (calc_pd_test) {Calculate Test PDs \\ (for dim0 \& dim1)};

% == Column 3: Bayesian Model Training ==
% Position this block relative to the training data processing nodes
\node[process, right=of calc_pd_train] (likelihoods) {Likelihood Surfaces from Train PDs \\ (for digits 0-9)};
%\node[connector, right=of likelihoods] (update_op) {$\otimes$};
\node[connector, right=of likelihoods] (update_op) {$\odot$};
\node[prior, right=of update_op] (priors) {Uninformative Priors \\ (for digits 0-9)};
\node[posterior, below=of update_op] (posteriors) {Posterior Surfaces \\ (for digits 0-9)};

% Bounding box for the Bayesian update process
\node[draw, dashed, inner sep=0.4cm, fit=(priors) (likelihoods) (update_op) (posteriors), label={[font=\sffamily\bfseries]above:Bayesian Update}] (model_box) {};

% == Column 4: Classification ==
% Position the classification node vertically centered between its inputs for a balanced look
\node[process, below=of posteriors] (calc_dist) {Calculate Distances to all Posteriors \\ Distance = $d_{0} + d_{1}$};
\node[result, below=of calc_dist] (classify) {Classify as \\ argmin(Distance)};

% == Arrows ==
% Connect nodes with clearer, non-overlapping paths
\draw[arrow] (train) -- (calc_pd_train);
\draw[arrow] (test) -- (calc_pd_test);

% Bayesian model flow
\draw[arrow] (calc_pd_train) -- (likelihoods);
\draw[arrow] (priors) |- (posteriors);
\draw[arrow] (likelihoods) |- (posteriors);

% Classification flow
% Use |- routing to different anchors (north west and south west) to keep lines clean
\draw[arrow] (posteriors) -- (calc_dist);
\draw[arrow] (calc_pd_test) -- (calc_dist);

% Arrow with a nicely placed label for the distance formula
\draw[arrow] (calc_dist) -- (classify);
    %node[edge_label, right=0.2cm] {Distance = \\ $(1-\lambda)d_{0} + \lambda d_{1}$};
    %node[edge_label, right=0.2cm] {Distance = $d_{0} + d_{1}$};
    
\end{tikzpicture}
```

# Analysis {#sec-Analysis}

## ML Analysis

*Need confusion matrices*
*Need accuracy, precision, recall, f_meas*

```{r ml_results}
#run the Mnist ML method code first, and take results from there
# ml_results <- tibble(
#   method = c("multinomial", "dropout nn", "ridge nn", "lasso no"),
#   accuracy = c(mlogit_acc, nn_dropout_accu, nn_ridge_accu, nn_lasso_accu)
# )
# saveRDS(ml_results, "ml_results")

ml_results <- readRDS("ml_results") #still a tibble so easy to edit

ml_results |> 
  kable(digits = 4)
```

## Proposed Method Analysis

```{r post_digit_plot0, fig.dim=c(10,6)}
posterior_list_dim0 <- readRDS("btda/posterior_list_dim0.rds")
posterior_dim0_df <- posterior_list_dim0 |>
  set_names(0:9) |>
  list_rbind(names_to = "digit")

ggplot(posterior_dim0_df, aes(x = birth, y = persistence, fill = intensity)) +
  geom_raster() +
  facet_wrap(~digit, ncol = 5) +
  scale_fill_viridis_c(option = "magma") +
  labs(
    title = "Posterior Densities for Digit Components (Dimension 0)",
    x = "Birth",
    y = "Persistence",
    fill = "Intensity"
  ) +
  theme_minimal() +
  theme(strip.text = element_text(size = 12, face = "bold"))
```

## Proposed Method Analysis

```{r post_digit_plot1, fig.dim=c(10,6)}
posterior_list_dim1 <- readRDS("btda/posterior_list_dim1.rds")
posterior_dim1_df <- posterior_list_dim1 |>
  set_names(0:9) |>
  list_rbind(names_to = "digit")

ggplot(posterior_dim1_df, aes(x = birth, y = persistence, fill = intensity)) +
  geom_raster() +
  facet_wrap(~digit, ncol = 5) +
  scale_fill_viridis_c(option = "magma") +
  labs(
    title = "Posterior Densities for Digit Loops (Dimension 1)",
    x = "Birth",
    y = "Persistence",
    fill = "Intensity"
  ) +
  theme_minimal() +
  theme(strip.text = element_text(size = 12, face = "bold"))
```

## Proposed Method Analysis

```{r tda01confmatrix, fig.dim=c(10,6)}

full_tda_01_results <- readRDS("btda/full_tda_01_results.rds")

conf_matrix <- table(
  true_label = full_tda_01_results$true_label,
  predicted_label = full_tda_01_results$predicted_label
)

as.data.frame.matrix(conf_matrix) %>%
  mutate(true_label = rownames(.)) %>%
  pivot_longer(
    cols = -true_label,
    names_to = "predicted_label",
    values_to = "count"
  ) |>
ggplot(aes(x = predicted_label, y = true_label, fill = count)) +
  geom_tile(color = "white") +
  scale_fill_viridis_c(option = "magma", direction = 1) +
  theme_minimal() +
  labs(
    x = "Predicted Label",
    y = "True Label",
    fill = "Count"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# accuracy <- mean(full_results$true_label == full_results$predicted_label, na.rm = TRUE)

```

## Proposed Method Analysis

```{r tda01metrics}
multi_metrics <- metric_set(
  yardstick::accuracy, 
  yardstick::precision, 
  yardstick::recall, 
  yardstick::f_meas
)

# Now this will work correctly
results_for_metrics <- full_tda_01_results |>
  select(true_label, predicted_label) |>
  mutate(
    true_label = factor(true_label, levels = 0:9),
    predicted_label = factor(predicted_label, levels = 0:9)
  )

multi_metrics(results_for_metrics, truth = true_label, estimate = predicted_label) |> 
  janitor::clean_names() |>
  kable()
```

# TDA + ML {#sec-TDAML}

# Results/Future Work {#sec-Results}



## ML + TDA Results

# References {#sec-References}

## References

::: {#refs .smaller}
:::
