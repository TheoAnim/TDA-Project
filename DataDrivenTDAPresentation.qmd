---
title: "The Shape of Digits"
subtitle: "A Bayesian Topological Data Analytic Approach to Classification of Handwritten Digits"
authors:
  - name: Thomas Reinke
    affiliation: 
      - name: Baylor University
        department: Statistical Science
        # city: Waco
        # state: TX
        # country: US
        url: https://www.baylor.edu
    # email: thomas_reinke1@baylor.edu
  - name: Theophilus A. Bediako
    affiliation: 
      - name: Baylor University
        department: Statistical Science
        # city: Waco
        # state: TX
        # country: US
        url: https://www.baylor.edu
  - name: Daniel Lim
    affiliation: 
      - name: Baylor University
        department: Statistical Science
        # city: Waco
        # state: TX
        # country: US
        url: https://www.baylor.edu
date: today
date-format: "MMMM D, YYYY"
format: 
  revealjs:
    theme: 
      - quarto-assets/baylor-theme.scss
    smaller: false
    scrollable: false
    show-slide-number: all
    toc: false
    toc-depth: 1
    preview-links: true
    slide-number: c/t
    multiplex: false
    embed-resources: true
    auto-animate: true
    #footer: "Thomas Reinke"
bibliography: references.bibtex
lightbox:
  match: auto
  effect: fade
  desc-position: bottom
  loop: true
logo: "quarto-assets/baylor.png"
license: "CC BY-NC"
copyright: 
  holder: Thomas Reinke
  year: 2025
editor: 
  markdown: 
    wrap: 72
fig-width: 15
---

```{r, setup}
#| include: false
#| message: false
library(quarto)
library(knitr)
library(tidyverse)
library(conflicted)
library(janitor)
library(ggtda)
# library(TDAvis)
library(patchwork)
library(gganimate)
library(ggforce)
library(simplextree) 
library(gifski)
library(magick)  
library(ripserr)
library(reshape2)
# remotes::install_github("maroulaslab/BayesTDA") Use this if package ‘BayesTDA’ is not available for this version of R
library(BayesTDA)
library(TDAstats)
library(mvtnorm)
library(kableExtra)
library(plotly)
library(DiagrammeR)
conflicted::conflict_prefer("filter", "dplyr")
conflicted::conflict_prefer("select", "dplyr")
conflicted::conflicts_prefer(ggtda::geom_simplicial_complex)
conflicted::conflicts_prefer(plotly::layout)
knitr::opts_chunk$set(
  comment = "#>",
  message = FALSE,
  warning = FALSE,
  cache = FALSE,
  echo = FALSE,
  tidy.opts = list(width.cutoff = 100),
  tidy = FALSE,
  fig.align = "center"
)
ggplot2::theme_set(ggplot2::theme_minimal())
ggplot2::theme_update(panel.grid.minor = ggplot2::element_blank())

#------------------------------------------------------------#
```

::: {.content-hidden}
$$
{{< include quarto-assets/_macros.tex >}}
$$
:::

# Contents

1. [MNIST Overview](#sec-MNIST)
1. [Machine Learning/Classification Methods](#sec-MLMethod)
1. [Intro to TDA](#sec-TDAIntro)
1. [TDA Methodology](#sec-TDAMethod) <!--Bayes TDA Methodology-->
1. [Analysis](#sec-Analysis)
1. [Results/Future Work](#sec-Results)
1. [References](#sec-References)

# MNIST Overview {#sec-MNIST}

## Data Overview

## Example digit

```{r}
# mnist <- dslabs::read_mnist()
# saveRDS(mnist, file = "mnist_dataset")
mnist <- readRDS(file = "mnist_dataset")

train <- mnist$train
test <- mnist$test
```


```{r}
train_images <- train$images  # Matrix of size 60,000 x 784
train_labels <- as.factor(train$labels)  # Factorized labels (0-9)

test_images <- test$images   # Matrix of size 10,000 x 784
test_labels <- as.factor(test$labels)

train_images <- train_images / 255
test_images <- test_images / 255

train_images_list <- lapply(1:nrow(train_images), function(i) {
  matrix(train_images[i, ], nrow = 28) |> t()
})

test_images_list <- lapply(1:nrow(test_images), function(i) {
  matrix(test_images[i, ], nrow = 28) |> t()
})

```


```{r}
plot_digit <- \(image_list = train_images_list, image_index = NULL, image_df = NULL, melted = FALSE){
  if(!melted){
    image_df <- melt(image_list[image_index])
    colnames(image_df) <- c("y", "x", "value")
  }
  ggplot(image_df, aes(x = x, y = y, fill = value)) + 
    geom_raster() +
  scale_fill_gradient(low = "white", high = "black") +
  scale_y_reverse() + 
  coord_equal() +
  theme_void() + 
  theme(legend.position = "none") 
}

plot_digit(image_index = 8)
train$labels[8]
```

## Binarization


```{r}
binarize_images <- function(images_list, threshold = 0.5) {
  lapply(images_list, function(mat) {
    ifelse(mat < threshold, 0, 1)
  })
}

train_images_binarized <- binarize_images(train_images_list)

plot_digit(image_index = 8) + plot_digit(train_images_binarized, image_index = 8)

```

# Machine Learning & Classification {#sec-MLMethod}

## Machine Learning Methods 
:::{.columns}

::: {.column}
:::{.fragment}
- Naive Bayes  
- K-Nearest Neighbor
:::
:::

::: {.column}
:::{.fragment}
- Random Forest  
- Neural Network
:::
:::

:::

## Previous TDA Work

:::{.fragment}
* A Bayesian Framework for Persistent Homology[@Maroulas2020-sp]
* Classification based on Topological Data Analysis[@DBLP:journals/corr/abs-2102-03709]
* A topological data analysis based classification method for multiple measurements[@DBLP:journals/corr/abs-1904-02971]
* Topology based data analysis identifies a subgroup of breast cancers with a unique mutational profile and excellent survival[@doi:10.1073/pnas.1102826108]
* A Topological "Reading" Lesson: Classification of MNIST using TDA[@DBLP:journals/corr/abs-1910-08345]
:::


# Intro To TDA {#sec-TDAIntro}

# 

:::{.callout-quote}
"Data has shape,  
shape has meaning,  
meaning brings value."  
- Gunnar Carlsson
:::

## Introductory TDA

<!-- :::{.incremental} -->
<!-- - Explore topological structure in datasets -->
<!--     - Persistent homolgy, associate shapes to data & summarize features -->
<!--     - Persitence diagrams, Mapper, etc. -->
<!-- ::: -->

:::{.incremental}
- Applications in many areas
    - handwriting analysis, brain arteries, image analysis, neuroscience, sensor networks, protein structures, biology, dynamical systems, action recognition, signal analysis, chemistry, genetics, object data, etc.
:::

:::{.fragment}
- Dimension reduction by topological features
    - Analogous to statistic computed from data
:::

## Motivating Example

```{r ex_df, fig.dim=c(6,6)}
set.seed(3051936)

p1 <- tdaunif::sample_circle(n = 40L, sd = .05)
p2 <- tdaunif::sample_circle(n = 20L, sd = .025) / 2
colnames(p1) <- colnames(p2) <- c("x", "y")
p2[, "x"] <- p2[, "x"] + .5

df <- rbind(
  cbind(as.data.frame(p1), lab = "a"),
  cbind(as.data.frame(p2), lab = "b")
)

ggplot(df, aes(x, y)) +
  geom_point() +
  coord_fixed()
```

## Introductory TDA

:::{.fragment}
* Create mesh of the data that approximates the true surface/structure we are sampling from
:::

:::{.fragment}
* **Simplices:** Basic shapes like points, line segments, triangles, tetrahedra
:::

:::{.fragment}
* **Simplicial Complexes:** Collections of simplices "glued" together in a structured way
:::



<!-- ## Simplices: Geometric Building Blocks {.smaller} -->

<!-- * A **$k$-simplex** is the convex hull of $k+1$ geometrically independent points (vertices) $\{v_0, ..., v_k\}$. -->
<!--     * Notation: $[v_0, ..., v_k] = \bigl\{ \sum_{i=0}^k \alpha_i v_i : \sum_{i=0}^k \alpha_i = 1, \alpha_i \ge 0 \bigr\}$ -->

<!-- ::: {.incremental} -->
<!-- * **Examples:** -->
<!--     * 0-simplex: $[v_0]$ (a point/vertex) -->
<!--     * 1-simplex: $[v_0, v_1]$ (a line segment/edge) -->
<!--     * 2-simplex: $[v_0, v_1, v_2]$ (a triangle, including interior) -->
<!--     * 3-simplex: $[v_0, v_1, v_2, v_3]$ (a tetrahedron, including interior) -->
<!-- ::: -->

<!-- :::{.fragment} -->
<!-- * **Faces:** k-1 simplices spanned by subsets of the vertices. (e.g., edges of a triangle). -->
<!-- ::: -->

<!-- :::{.notes} -->
<!-- - $\alpha_i$ is weights/coefficients that sum to one, creates the hull/filled-in geometric object -->
<!-- - The convex hull is the smallest convex set that encloses all the points, forming a convex polygon -->
<!-- - "geometrically independent" means the points are affinely independent -->
<!-- - ex 3 points, not collinear -->
<!-- ::: -->


<!-- ## Simplicial Complexes -->

<!-- A **Simplicial Complex** $S$ is a collection of simplices satisfying: -->

<!-- :::{.fragment} -->
<!-- 1.  **Face Inclusion:** If a simplex $\xi \in S$, then all faces of $\xi$ are also in $S$. -->
<!-- ::: -->
<!-- :::{.incremental} -->
<!-- 2.  **Consistent Intersection:** The intersection of any two simplices $\xi_1, \xi_2 \in S$ is either: -->
<!--     * Empty ($\emptyset$), or -->
<!--     * Contained in $S$ -->
<!-- ::: -->

<!-- :::{.fragment} -->
<!-- * **Intuition**: A well-constructed mesh made of points, edges, triangles, etc. -->
<!-- ::: -->


## Vietoris-Rips

:::{.incremental}
* **Goal:** Build a simplicial complex from a point cloud $X = \{x_i\}_{i=1}^L \subset \mathbb{R}^d$.
* **Vietoris-Rips Complex $\mathcal{V}_r(X)$:**
    * Parameter: Proximity radius $r > 0$.
    * Rule: A simplex $[x_{i_1}, ..., x_{i_l}] \in \mathcal{V}_r(X)$ iff $\text{diam} (x_{i_1}, ..., x_{i_l}) \leq r$
:::

:::{.fragment}
* **Intuition:** Connect groups of points that are close to each other.
:::

:::{.notes}
- $\text{diam}$ means the greatest distance between any 2 points
:::

## Example 

<!-- This is the simplicial complex generated by $r = .25$ -->

```{r ex_sc, fig.align='center', fig.dim=c(10,6)}

radius <- .275

prox <- 2 * radius

p_d <- ggplot(df, aes(x = x, y = y)) +
  coord_fixed() +
  geom_disk(radius = radius, fill = "aquamarine3") +
  geom_point() +
  theme_bw()

plot1 <- ggplot(df, aes(x, y)) +
  geom_simplicial_complex(radius = .275) +
  coord_fixed()

plot2 <- ggplot(df, aes(x, y)) +
  geom_simplicial_complex(
    mapping = aes(fill = after_stat(face)),
    dimension_max = 4L,
    alpha = .25, radius = .275
  ) +
  coord_fixed()

# p3 <- ggplot(df, aes(x, y, fill = lab)) +
#   geom_simplicial_complex(radius = .3) +
#   coord_fixed()
# 
# p4 <- ggplot(df, aes(x, y)) +
#   stat_simplicial_complex(
#     mapping = aes(fill = after_stat(factor(dimension)),
#                   color = after_stat(factor(dimension))),
#     dimension_max = 6L,# one_simplices = "all",
#     alpha = .5, radius = .3
#   ) +
#   facet_wrap(vars(lab), ncol = 2) +
#   scale_fill_viridis_d(direction = -1, aesthetics = c("color", "fill")) +
#   coord_fixed()

p_d + plot1

# p1 + p2 + p3 + p4 +
#   plot_layout(
#     nrow = 2,
#     ncol = 2
#   )
  

```


## VR Filtration

:::{.fragment}
* Given a nondecreasing sequence $\{ r_n\}  \in  \mathbb{R}^+ \cup  \{ 0\}$  with $r_0 = 0$, we denote its Vietoris--Rips filtration by $\{ \mathcal{V}_{r_n} (X)\}_{ n\in \mathbb{N}}$ .
:::

:::{.fragment}
* A **Vietoris-Rips Filtration** is a sequence of nested simplicial complexes:
    $$ \mathcal{V}_{r_0}(X) \subseteq \mathcal{V}_{r_1}(X) \subseteq \mathcal{V}_{r_2}(X) \subseteq \dots $$
:::

:::{.fragment}
* As we increase the radius $r$, more points get connected, and higher-dimensional simplices appear.    
:::

:::{.fragment}
* This filtration tracks how the topological structure (connectivity, holes, voids) evolves as the scale $r$ changes.
:::

## VR Visualization

```{r vr_vis}
# Define the sequence of radii
# radii_seq <- seq(0, .35, length.out = 201)
# 
# create_plot <- function(r, data) {
#   ggplot(data, aes(x = x, y = y)) +
#     coord_fixed() +
#     geom_disk(radius = r, fill = "aquamarine3", alpha = 0.5) +
#     geom_simplicial_complex(radius = r) +
#     geom_point() +
#     labs(title = paste("Radius:", format(r, digits = 2))) +
#     theme_bw()
# }
# 
# plot_list <- map(radii_seq, ~ create_plot(.x, data = df))
# 
# temp_dir <- tempdir()
# file_paths <- file.path(temp_dir, paste0("frame_", seq_along(plot_list), ".png"))
# 
# pwalk(list(plot_list, file_paths),
#       ~ ggsave(filename = ..2, plot = ..1, width = 10, height = 5, dpi = 500))
# 
# img_list <- map(file_paths, image_read)
# 
# img_joined <- image_join(img_list)
# 
# animation <- image_animate(img_joined, fps = 20)
# 
# # print(animation)
# 
# output_gif_path <- "radius_animation.gif"
# image_write(animation, path = output_gif_path)
# 
# unlink(file_paths)

```

![](radius_animation.gif)

## Persistence Diagrams {.smaller}

:::{.incremental}
* A Persistence Diagram (PD) $\mathcal{D}$ is a multiset of points in the space $\mathcal{W} = \mathbb{W} \times \{0, 1, ..., D-1\}$.
    * The **birth-death plane** is $\mathbb{W} = \{ (b, d) \in \mathbb{R}^2 \mid d \ge b \ge 0 \}$.
:::

:::{.incremental}
* Each point $(b, d, k) \in \mathcal{D}$ represents:
    * A homological feature of dimension $k$.
    * The feature appears (is **b**orn) at filtration scale $b$.
    * The feature disappears ( **d**ies) at filtration scale $d$.
:::

:::{.incremental}
* **Persistence:** $d-b$ (how long the feature "persists"). Longer persistence often indicates significant features.
* **Dimensions:**
    * $k=0$: Connected components.
    * $k=1$: Loops or holes.
    * $k=2$: Voids or cavities.
:::

:::{.notes}
A **Persistence Diagram** $\mathcal{D}$ summarizes the topological features found in a filtration.
- homological feature dim $k$ appears at $b$ and disappears at $d$
- dies when merges with another feature, ex loop dies when 'filled in' by triangles
- void - empty sphere, like a basketball
:::

## Persistence Diagrams

```{r pd_diagram, fig.dim=c(10,6)}
rips_data <- ripserr::vietoris_rips(df[, c("x", "y")])

barcode_graph <- ggplot(rips_data) +
  geom_barcode(
    aes(start = birth,
        end = death,
        color = factor(dimension),
        linetype = factor(dimension)),
    linewidth = 1.2  
  ) +
  scale_color_viridis_d(end = .7, name = "Dimension") +
  scale_linetype_discrete(name = "Dimension") +
  labs(title = "Barcode")

pd_graph <- ggplot(rips_data) +
  stat_persistence(
    aes(start = birth, end = death,
        color = factor(dimension), shape = factor(dimension)),
    size = 2.5,
    alpha = .75
  ) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", alpha = 0.6) +
  scale_color_viridis_d(end = .7, name = "Dimension") +
  scale_shape_discrete(name = "Dimension") +
  labs(title = "Persistence Diagram")

barcode_graph + pd_graph +
  plot_layout(guides = 'collect') &
  theme(legend.position = 'bottom')
```

# Bayes TDA Methodology {#sec-TDAMethod}

## Methodology

```{dot}
//| fig-align: center
//| fig-width: 10

digraph flowchart {
  graph [fontname="Roboto", ratio="compress"];
  node [shape=box, style=rounded, fillcolor="#fefefe", fontname="Roboto", fontsize="12", color="#134e13"];
  
  edge [color="#134e13"];

  A [label="Data"];
  // Added a line break (\n) to the long label
  B [label="Filter Data \n Persistence Diagrams"];
  C [label=" Bayesian Marked\n Poisson Point Process "];
  D [label=" Classification "];

  A -> B;
  B -> C;
  C -> D;
}
```

## Marked Poisson Point Process {.smaller}

:::{.fragment}
- **Tilted Space:** Use $(b, d-b)$ instead of $(b, d)$, denoted $\mathbb{W}$. $\mathcal{D}$ denotes tilted PDs in $\mathbb{W}$.
- **Single Dimension:** Analyze one homological dimension $k$ at a time, assuming independence across dimensions.
:::

:::{.fragment}
- **Poisson Point Process** $\Pi$:
    * **Cardinality ($N$):** The number of points $N \sim \text{Poisson}(\mu = \Lambda(\mathbb{X}))$.
    * **Spatial Distribution ($\mathbb{P}_n$):** Given $N=n$ points, their locations are drawn **independently** from the normalized intensity measure $\frac{\Lambda(\cdot)}{\mu}$.
    * For $A \subseteq \mathbb{X}$ , $\mathbb{E}[|\Pi \cap A|] = \Lambda(A)$ 
:::

:::{.fragment}
- **Marked PPP**: Points in $\mathbb{X}$ with marks in $\mathbb{M}$, defined by:  
  - Base PPP on $\mathbb{X}$ with intensity $\Lambda$, and marks via kernel $\ell(x, \cdot)$ (conditionally independent given locations).
:::

<!-- ## Marked Poisson Point Processes {.smaller} -->

<!-- :::{.incremental} -->
<!-- - **Tilted Space:** Use $(b, d-b)$ instead of $(b, d)$, denoted $\mathbb{W}$. $\mathcal{D}$ denotes tilted PDs in $\mathbb{W}$.   -->
<!-- - **Single Dimension Focus:** Analyze one homological dimension $k$, assuming independence across dimensions. -->
<!-- ::: -->

<!-- :::{.incremental} -->
<!-- - **Poisson Point Process (PPP)**: A random collection of points in a space $\mathbb{X}$, defined by an **intensity measure** $\Lambda$. -->
<!-- - **Number of Points:** The total count $N$ follows a Poisson distribution, $N \sim \text{Poisson}(\mu = \Lambda(\mathbb{X})).$ -->
<!-- - **Point Locations:** Given $N = n$, the positions of the $n$ points are independently sampled from the **normalized intensity measure** $\Lambda / \mu$. -->
<!--   - If $\Lambda$ has a density $\lambda(x)$, the points are sampled as: $x_1, \dots, x_n \sim \frac{\lambda(x)}{\mu}.$ -->
<!--   - The expected number of points in any region $A \subseteq \mathbb{X}$ is $\mathbb{E}[|\Pi \cap A|] = \Lambda(A).$ -->
<!-- ::: -->

<!-- :::{.incremental} -->
<!-- - **Marked PPP:** Points $x \in \mathbb{X}$ with marks $m \in \mathbb{M}$, defined on $\mathbb{X} \times \mathbb{M}$. -->
<!--     - Base PPP on $\mathbb{X}$ with intensity $\Lambda$. -->
<!--     - Stochastic kernel $\ell(x, \cdot)$: Defines mark distribution conditional on location $x$. -->
<!--   - **Independence:** Marks $\{m_i\}$ are conditionally independent given locations $\{x_i\}$. -->
<!-- ::: -->

<!-- :::{.notes} -->
<!-- **Setting the Stage: Representation & Goal**   -->
<!-- - **Tilted Space ($\mathbb{W}$):** The coordinate transformation $(b, d) \rightarrow (b, p = d - b)$ isn’t just a convenience—it’s a strategic choice to focus on **persistence** $p$, which captures the "lifetime" of topological features. This simplifies modeling by decoupling birth times from death times, making patterns in persistence more interpretable.   -->
<!-- - **Model M1 (Single Dimension Focus):** Assuming independence across homological dimensions ($H_0, H_1, \dots$) is a critical simplification. It lets us isolate and analyze features in one dimension at a time, avoiding the complexity of joint dependencies. This assumption is foundational for building scalable inference methods.   -->
<!-- - **Notation Clarification:** The notation abuse ($\mathcal{D}_X$ instead of $\mathcal{D}^k_X$) is intentional to streamline the framework. It reflects that all operations are implicitly tied to a fixed dimension $k$, which is consistent with the paper’s approach.   -->
<!-- - **Bayesian Goal Context:** While not explicitly stated in this slide, the ultimate aim of building a Bayesian framework is to **infer latent PDs** ($\mathcal{D}_X$) from observed data ($\mathcal{D}_Y$) by leveraging PPPs. This ties the representation and assumptions to a broader inference strategy.   -->

<!-- **The Poisson Point Process (PPP)**   -->
<!-- - **Why PPPs for PDs?** Persistence diagrams are inherently random: they vary with data and contain a variable number of points. A PPP naturally models this by allowing **random counts** (via Poisson distribution) and **flexible spatial distributions** (via intensity $\Lambda$).   -->
<!-- - **Intensity Measure ($\Lambda$) as a Core Tool:** The intensity $\Lambda(A)$ isn’t just a mathematical abstraction—it’s the **expected number of points** in region $A$. This intuitive link between measure theory and practical modeling makes PPPs ideal for PD analysis.   -->
<!-- - **Normalization Trick:** The spatial distribution given $N = n$ is $\Lambda / \mu$, where $\mu = \Lambda(\mathbb{X})$. This ensures that the conditional distribution is valid (probabilities sum to 1) while preserving the structure of $\Lambda$.   -->
<!-- - **PPP vs. Fixed Datasets:** Unlike a fixed dataset, a PPP’s realization is random in both count and location. For PDs, this aligns with the idea that topological features are **stochastic**—they vary across datasets and need probabilistic modeling.   -->

<!-- **Marked Poisson Point Processes**   -->
<!-- - **Marks as Feature Attributes:** Marks ($m \in \mathbb{M}$) could encode properties like feature **significance**, **type** (e.g., "hole" vs. "component"), or **noise status**. This extends the PPP framework to model richer data, where locations and attributes are jointly random.   -->
<!-- - **Stochastic Kernel ($\ell$) as a Conditional Model:** The kernel $\ell(x, \cdot)$ defines how marks depend on location $x$. For example, in PDs, a point near the diagonal ($b \approx d$) might have a mark indicating "short-lived" features, while one far from the diagonal has a mark for "persistent."   -->
<!-- - **Conditional Independence in Marks:** Once locations are fixed, marks are independent. This simplifies inference and allows for modular modeling: you can first infer locations (using the base PPP) and then analyze marks separately.   -->
<!-- - **Bridge to Bayesian Inference:** The marked PPP structure is essential for the later Bayesian framework. Observed marks (e.g., feature properties) can inform updates to the latent PD ($\mathcal{D}_X$), making it a natural setting for likelihood-based inference.   -->

<!-- ::: -->

## Bayesian Framework: Analogy

|      | Bayesian framework for RVs | Bayesian framework for random PDs   |
| :------------- | :-------------------------------- | :---------------------------------------------------------- |
| **Prior** | Modeled by a prior density $f$    | Modeled by a Poisson PP with prior intensity $\lambda$        |
| **Likelihood** | Depends on observed data          | Stochastic kernel $\color{purple}{\ell(y|x)}$ depends on observed PDs |
| **Posterior** | Compute the posterior density     | Defines a Poisson PP with posterior intensity               |

<!-- :::{.fragment} -->
<!-- * **Core Idea:** Treat unknown, latent PD $\mathcal{D}^X$ as random draw from PPP defined by *prior* intensity $\lambda_{\mathcal{D}^X}$. Use observed PDs $D_{Y^1}, ..., D_{Y^m}$ (for fixed $k$) and likelihood kernel $\color{purple}{\ell(y|x)}$ to get *posterior* intensity $\lambda_{\mathcal{D}_X | D_{Y^{1:m}}}$. -->
<!-- ::: -->

::: {.notes}
**Speaker Notes:**

* This table draws parallels between standard Bayesian inference (for parameters $\theta$ of random variables) and our proposed framework for persistence diagrams (PDs). Remember, we're focusing on **one fixed homological dimension $k$**.
* **Prior Belief:**
    * *Standard:* We express prior belief about a parameter $\theta$ using a probability density function $f(\theta)$.
    * *PD Framework:* Our prior belief about the *latent* PD $\mathcal{D}^X$ (really $\mathcal{D}^k_X$) is encoded by modeling it as a **Poisson Point Process (PPP)**. The key characteristic of this PPP is its **prior intensity function $\lambda(x)$** (short for $\lambda_{\mathcal{D}^k_X}(x)$). This function tells us the expected density of "true" latent features at location $x=(b, p)$ in the persistence plane $\mathbb{W}$.
* **Data Connection (Likelihood):**
    * *Standard:* The likelihood function $L(\text{data}|\theta)$ quantifies how probable the observed data is, given a specific parameter value $\theta$.
    * *PD Framework:* The link between the latent features $x$ (from $\mathcal{D}^k_X$) and the observed features $y$ (in $\mathcal{D}^k_Y$) is defined by a **stochastic kernel** $\color{purple}{\ell(y|x)}$. This kernel acts like our likelihood: it gives the probability density of observing a feature $y$ *assuming* it originated from a specific latent feature $x$. The actual data we use are the points in the observed diagram(s) $D_{Y^1}, \dots, D_{Y^m}$. (This corresponds to the `purple` term in the theorem).
* **Updated Belief (Posterior):**
    * *Standard:* Combining the prior $f(\theta)$ and likelihood $L(\text{data}|\theta)$ via Bayes' theorem gives the posterior density $f(\theta|\text{data})$.
    * *PD Framework:* Combining the prior intensity $\lambda(x)$ and the likelihood kernel $\color{purple}{\ell(y|x)}$ using the observed data $D_{Y^{1:m}} = \cup_{i=1}^m D_{Y^i}$ yields the **posterior intensity function** $\lambda(x | D_{Y^{1:m}})$. This function defines a *posterior* Poisson Point Process, which represents our updated probabilistic belief about the latent PD $\mathcal{D}^k_X$ after seeing the data. (The formula involves the `red`, `blue`, `purple`, and `darkgreen` components).
* **Core Idea Summary:** We model the unknown true PD (for dimension $k$) as a PPP realization governed by a prior intensity $\lambda$. We use the actual points from observed diagrams ($D_{Y^1}, \dots, D_{Y^m}$, all for dim $k$) and a kernel $\color{purple}{\ell(y|x)}$ (our likelihood) to compute a posterior intensity. This posterior intensity defines the PPP representing our updated understanding of the true PD structure.
:::

<!-- ## The Model: Latent PD $\mathcal{D}_X$ {.smaller} -->

<!-- * The "true" underlying PD $\mathcal{D}_X$ is modeled as a **Poisson Point Process (PPP)** with a **prior intensity density** $\lambda_{\mathcal{D}_X}(x)$. -->

<!-- ::: {.incremental} -->
<!-- * Any potential feature $x$ from the prior might be **observable** or **vanish** (unobserved). -->
<!--     * $\alpha(x)$: Probability that feature $x$ is potentially observable. -->
<!-- * $\mathcal{D}_X$ is decomposed into two *independent* PPPs: -->
<!--     * $\mathcal{D}_{XO}$ (**O**bserved Track): Points that *could* generate an observation. -->
<!--         * Intensity: $\color{blue}{\alpha(x) \lambda_{\mathcal{D}_X}(x)}$ -->
<!--     * $\mathcal{D}_{XV}$ (**V**anished Track): Points that are missed / not observed. -->
<!--         * Intensity: $\color{red}{(1-\alpha(x)) \lambda_{\mathcal{D}_X}(x)}$ -->
<!-- ::: -->

<!-- ::: {.notes} -->
<!-- * This slide describes Model M2, which defines our prior belief about the **latent ("true") persistence diagram**, $\mathcal{D}^X$. -->
<!-- * **Remember the Context (M1):** We are operating under Model M1, meaning we've fixed a specific **homological dimension $k$**. So, $\mathcal{D}_X$ here is technically $\mathcal{D}^k_X$, representing the true features only for that dimension $k$. -->
<!-- * **Prior Model:** We model this latent $\mathcal{D}_X$ as a **Poisson Point Process (PPP)** on the persistence plane $\mathbb{W}$. The behavior of this PPP is governed by its **prior intensity density** $\lambda_{\mathcal{D}_X}(x)$ (really $\lambda_{\mathcal{D}^k_X}(x)$). This function reflects our prior assumptions about the density of true dimension-$k$ features at location $x=(b, p)$. -->
<!-- * **Detectability** $\alpha(x)$: We acknowledge that not all true features might be detectable in real data. We introduce $\alpha(x)$, the probability that a true feature at location $x$ is *potentially observable*. Conversely, $1-\alpha(x)$ is the probability that it *vanishes* (is missed entirely, perhaps due to low persistence, noise, or sampling issues). $\alpha(x)$ models feature detectability. -->
<!-- * **Decomposition:** Using $\alpha(x)$, we can think of the original latent process $\mathcal{D}^X$ as being composed of two independent PPPs (this is a standard PPP property called "thinning"): -->
<!--     * $\mathcal{D}_{XO}$ **(Observable Track):** The subset of latent features that *could* potentially generate an observation in our data. Its intensity is the prior intensity modulated by the probability of being observable: $\color{blue}{\alpha(x) \lambda_{\mathcal{D}_X}(x)}$. This is the "signal" part of our prior that can interact with data (related to the `blue` term in the theorem's denominator). -->
<!--     * $\mathcal{D}^{XV}$ **(Vanished Track):** The subset of latent features that are fundamentally unobservable. Its intensity is modulated by the probability of vanishing: $\color{red}{(1-\alpha(x)) \lambda_{\mathcal{D}_X}(x)}$. This part of our prior belief cannot be updated by observations (this is the `red` term in the theorem). -->
<!-- * **Consistency:** Note that the intensities sum correctly: $\color{blue}{\alpha(x)\lambda_{\mathcal{D}_X}(x)} + \color{red}{(1-\alpha(x))\lambda_{\mathcal{D}_X}(x)} = \lambda_{\mathcal{D}_X}(x)$, recovering the total prior intensity. -->
<!-- ::: -->

<!-- ## The Model: Observed PD $\mathcal{D}_Y$ {.smaller} -->

<!-- * An *observed* PD $\mathcal{D}_Y$ is also decomposed into two *independent* components: -->

<!-- ::: {.incremental} -->
<!-- * $\mathcal{D}_{YO}$ (**O**bserved from Signal): Points generated *from* the latent observable points $\mathcal{D}_{XO}$. -->
<!--     * The pair $(\mathcal{D}_{XO}, \mathcal{D}_{YO})$ forms a **Marked PPP**. -->
<!--     * Connection is via the **stochastic kernel** $\color{purple}{\ell(y|x)}$ (the "likelihood"). It gives the probability density of observing $y \in \mathcal{D}_{YO}$ given a latent point $x \in \mathcal{D}_{XO}$. -->
<!-- * $\mathcal{D}_{YS}$ (**S**purious / Noise): Points arising independently from noise, clutter, or unanticipated geometry. -->
<!--     * Modeled as an independent PPP with intensity $\color{darkgreen}{\lambda_{\mathcal{D}_{YS}}(y)}$. -->
<!-- ::: -->

<!-- ::: {.notes} -->
<!-- * Now we turn to Model M3, which describes the structure of the **observed persistence diagrams** $\mathcal{D}_Y$. -->
<!-- * **Context:** Again, this is for our **fixed dimension** $k$. So $\mathcal{D}_Y$ means $\mathcal{D}^k_Y$. Crucially, $\mathcal{D}_Y$ represents the *random process* that generates the actual diagrams $D_{Y^1}, \dots, D_{Y^m}$ which we compute from our data. M3 models the underlying mechanism producing these diagrams. -->
<!-- * **Decomposition:** The model assumes that any observed diagram (a realization of $\mathcal{D}_Y$) is composed of points from two independent sources: -->
<!--     * 1. $\mathcal{D}_{YO}$ **(Observed from Signal):** These are the points in the observed diagram that correspond to the "true", potentially observable latent features we defined in M2 (the points in $\mathcal{D}_{XO}$). -->
<!--         * **Marked PPP:** The link between the latent signal $\mathcal{D}_{XO}$ and the observed signal $\mathcal{D}_{YO}$ is modeled as a **Marked Poisson Point Process**. Think of it this way: nature selects a latent feature $x$ to be potentially observable (it's in $\mathcal{D}_{XO}$). Then, the observation process generates a corresponding observed feature $y$ (which becomes a point in $\mathcal{D}_{YO}$) according to some probability distribution. The observed point $y$ is the "mark" associated with the latent point $x$. -->
<!--         * **Stochastic Kernel** $\color{purple}{\ell(y|x)}$: This kernel defines the probability density of the mark $y$ given the original latent point $x$. This $\color{purple}{\ell(y|x)}$ acts as our **likelihood function** (the `purple` term in the theorem). It models how the process of generating data and computing the PD might shift or perturb a true feature's location from $x$ to $y$. -->
<!--     * 2. $\mathcal{D}_{YS}$ **(Spurious / Noise):** These are points appearing in the observed diagram that do *not* originate from any true latent feature in $\mathcal{D}_X$. They arise independently from sources like measurement noise, computational artifacts, sampling variations, or other unmodeled aspects. -->
<!--         * This noise component is modeled as an independent **PPP** governed by its own intensity function $\color{darkgreen}{\lambda_{\mathcal{D}_{YS}}(y)}$. This intensity describes the expected spatial density of purely spurious points appearing near $y$ in an observed diagram. It's our background noise model for dimension $k$ (the `darkgreen` term in the theorem). -->
<!-- * **Interpretation:** According to M3, any specific point $y$ that we see in one of our actual computed diagrams (say, $D_{Y^i}$) could have either been generated by some latent feature $x$ (via $\color{purple}{\ell(y|x)}$) or it could be purely noise (generated according to $\color{darkgreen}{\lambda_{\mathcal{D}_{YS}}(y)}$). The Bayesian theorem will weigh these possibilities. -->
<!-- * *(Refer to Figure 4 if available):* Figure 4 visually summarizes these relationships: $\mathcal{D}_X$ (latent) splits into $\mathcal{D}_{XO}$ (latent signal) and $\mathcal{D}_{XV}$ (latent vanished). $\mathcal{D}_Y$ (observed process) splits into $\mathcal{D}_{YO}$ (observed signal) and $\mathcal{D}_{YS}$ (observed noise). Only $\mathcal{D}_{XO}$ and $\mathcal{D}_{YO}$ are directly linked via the likelihood kernel $\color{purple}{\ell(y|x)}$. -->
<!-- ::: -->

## Bayes' Theorem for PDs {.smaller}

* **Goal:** Find the **posterior intensity** $\lambda_{\mathcal{D}_X | D_{Y^{1:m}}}(x)$ for the latent PD $\mathcal{D}_X$, given $m$ independent observed PDs $D_{Y^1}, \dots, D_{Y^m}$.
* Let $D_{Y^{1:m}} = \cup_{i=1}^m D_{Y^i}$. The posterior intensity is:

::: {.smaller-equation}
$$
\lambda_{\mathcal{D}_X | D_{Y^{1:m}}}(x) = \underbrace{\color{red}{(1 - \alpha(x))\lambda_{\mathcal{D}_X}(x)}}_{\text{Prior Vanished Part}} + \underbrace{\frac{1}{m} \alpha(x) \sum_{i=1}^m \sum_{y \in D_{Y^i}} \frac{ \color{purple}{\ell(y|x)}\color{blue}{ \lambda_{\mathcal{D}_X}(x)}}{\color{darkgreen}{\lambda_{\mathcal{D}_{Y_S}}(y)} + \color{blue}{\int_{\mathbb{W}} \ell(y|u) \alpha(u) \lambda_{\mathcal{D}_X}(u) du}}}_{\text{Update from Observed Points } y} \quad \text{a.s.}
$$
:::

::: {.notes}
This formula gives the posterior intensity $\lambda_{\mathcal{D}_X | D_{Y^{1:m}}}(x)$, which represents our updated belief (as an expected density) about the presence of a latent feature at location $x \in \mathbb{W}$, given the observed data $D_{Y^{1:m}}$. It combines the prior belief with information from the observations. Let's break it down:

* **Overall Structure:** Posterior Intensity = Prior Vanished Part + Update from Observations
* **Term 1: Prior Vanished Part** $\color{red}{(1 - \alpha(x)) \lambda_{D_X(x)}}$
    * $\lambda_{\mathcal{D}_X}(x)$: Prior intensity density at $x$ (from M2).
    * $\alpha(x)$: Probability that a feature at $x$ is potentially observable (from M2).
    * $(1 - \alpha(x))$: Probability that a feature at $x$ is inherently unobservable (vanished, part of $\mathcal{D}^{XV}$).
    * This entire term $(\color{red}{red})$ represents the contribution from the part of the prior belief ($\mathcal{D}^{XV}$) that can **never** be informed by observations. It passes directly to the posterior unchanged.
* **Term 2: Update from Observed Points** $y$ ($\frac{1}{m} \sum ...$ part)
    * This term adjusts the prior belief about *potentially observable* features based on the actual observations $D_{Y^{1:m}}$.
    * $\frac{1}{m}$: Averages the update contribution across the $m$ independent observed diagrams.
    * $\sum_{i=1}^m \sum_{y \in D_{Y^i}}$: Sums the influence of *every observed point* $y$ from *all* $m$ observed diagrams $D_{Y^i}$.
    * **The Fraction:** This core part weights the influence of each observed $y$ on the posterior belief at $x$.
        * **Numerator** $\alpha(x) \ell(y|x) \lambda_{\mathcal{D}_X}(x)$: (Note: $\alpha(x)$ is outside sum in formula, but conceptually linked here)
            * $\color{blue}{\alpha(x) \lambda_{D_X(x)}}$: This is the **prior expected density of observable features specifically at location** $x$. (From M2).
            * $\color{purple}{\ell(y|x)}$: The likelihood kernel (from M3). How well does latent $x$ explain observed $y$? ($\color{purple}{purple}$)
            * The numerator combines the prior belief about observable features at $x$ (via $\alpha(x)$ and $\lambda_{\mathcal{D}_X}(x)$) with the likelihood of $y$ given $x$.
        * **Denominator:** Represents the total expected density of observing a point at location $y$, considering all possibilities:
            * $\color{darkgreen}{\lambda_{D_{Y_S}}(y)}$: Intensity of the spurious/noise process at $y$ (from M3). How likely is $y$ to be just noise? (\color{darkgreen}{darkgreen})
            * $\color{blue}{\text{integral over } W \text{ of } \ell(y|u) \alpha(u) \lambda_{D_X}(u) du}$: The marginal density of observing $y$ generated from *any* potentially observable latent feature $u \in \mathcal{D}^{XO}$. The term \color{blue}{alpha(u)lambda_D_X(u)} inside represents the **prior expected density of observable features at a *generic location u***. ($\color{blue}{ \int}$)
        * The fraction represents the contribution of observation $y$ to updating the belief at $x$, normalized by the total evidence for $y$.
    * **Putting the Update Term Together:** The averaged sum ($\frac{1}{m} \sum \sum ...$, multiplied by $\alpha(x)$) combines the weighted evidence from all observations $y$.
* **What is the Posterior?** The result $\lambda_{\mathcal{D}_X | D_{Y^{1:m}}}(x)$ is the intensity function of a *new* Poisson Point Process. This PPP represents our updated probabilistic model for the latent ("true") persistence diagram $\mathcal{D}_X$ after observing the data $D_{Y^{1:m}}$.
* **Why "almost surely" (a.s.)?**
    * This theorem describes a relationship between *random objects*. The observed diagrams $D_{Y^1}, \dots, D_{Y^m}$ are realizations of random point processes ($\mathcal{D}_Y$). Consequently, the posterior intensity $\lambda_{\mathcal{D}_X | D_{Y^{1:m}}}(x)$ is itself a *random function* because its value depends on this random data.
    * "Almost surely" is a standard term in probability theory meaning the equality holds *with probability 1*. It asserts that for any specific set of observations $D_{Y^{1:m}}$ you might get (excluding zero-probability outcomes), this formula correctly describes the posterior intensity.
:::

## Bayes PPP Example

```{r bayes_ppp_ex, fig.dim=c(10,6)}
#|cache: true
generate_pd <- function(n_points = 100, noise_variance, seed) {
  set.seed(seed)
  theta <- runif(n_points, 0, 2 * pi)
  point_cloud_tbl <- tibble(x = cos(theta), y = sin(theta)) |>
    mutate(
      x = x + rnorm(n(), 0, sqrt(noise_variance)),
      y = y + rnorm(n(), 0, sqrt(noise_variance))
    )
  homology_res <- TDAstats::calculate_homology(
    as.matrix(point_cloud_tbl), dim = 1, threshold = 2
  )
  
  as_tibble(homology_res) |>
    filter(dimension == 1, is.finite(death)) |>
    mutate(persistence = death - birth) |>
    select(dimension, birth, persistence)
}

calculate_prior_data <- function(grid_points, prior_params) {
  intensity_values <- apply(grid_points, 1, function(p) {
    if (p["persistence"] < 0) return(0)
    BayesTDA::Wedge_Gaussian_Mixture(
      x = as.numeric(p),
      weights = prior_params$weights,
      means = prior_params$means,
      sigmas = prior_params$sigmas
    )
  })
  intensity_values[!is.finite(intensity_values)] <- 0
  bind_cols(grid_points, intensity = intensity_values / max(intensity_values))
}

calculate_posterior_data <- function(grid_points, observed_pd, prior_params, noise_sigma, alpha, sigma_y) {
  Dy_list <- split(observed_pd |> select(birth, persistence) |> as.matrix(), 1:nrow(observed_pd))
  noise_params <- list(weights = 1, means = list(c(0.5, 0)), sigmas = noise_sigma)
  
  intensity_values <- apply(grid_points, 1, function(p) {
    if (p["persistence"] < 0) return(0)
    BayesTDA::postIntensityPoisson(
      x = as.numeric(p), Dy = Dy_list, alpha = alpha,
      weight.prior = prior_params$weights, mean.prior = prior_params$means,
      sigma.prior = prior_params$sigmas, sigma.y = sigma_y,
      weights.unexpected = noise_params$weights, mean.unexpected = noise_params$means,
      sigma.unexpected = noise_params$sigmas
    )
  })
  intensity_values[!is.finite(intensity_values)] <- 0
  bind_cols(grid_points, intensity = intensity_values / max(intensity_values))
}

plot_intensity <- function(data, observed = NULL, title = "") {
  p <- ggplot(data, aes(x = birth, y = persistence)) +
    geom_raster(aes(fill = intensity), interpolate = TRUE) +
    scale_fill_viridis_c(option = "plasma", name = "Intensity", limits = c(0, 1)) +
    coord_cartesian(xlim = c(0, 2.5), ylim = c(0, 2.5), expand = FALSE) +
    labs(x = "birth", y = "persistence", title = title) +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5))
    
  if (!is.null(observed)) {
    p <- p + geom_point(data = observed, color = "chartreuse3", size = 2.5, shape = 16)
  }
  p
}

observed_pd <- generate_pd(noise_variance = 0.001, seed = 123)

eval_grid <- expand_grid(
  birth = seq(0, 2.5, length.out = 100),
  persistence = seq(0, 2.5, length.out = 100)
)
prior_inf_params <- list(means = list(c(0.5, 1.2)), sigmas = c(.01), weights = c(1))
prior_uni_params <- list(means = list(c(1, 1)), sigmas = c(1), weights = c(1))

prior_inf_data <- calculate_prior_data(eval_grid, prior_inf_params)
prior_uni_data <- calculate_prior_data(eval_grid, prior_uni_params)

posterior_inf_data <- calculate_posterior_data(
  eval_grid, observed_pd, prior_inf_params,
  noise_sigma = sqrt(0.1), alpha = 1.0, sigma_y = 0.1
)
posterior_uni_data <- calculate_posterior_data(
  eval_grid, observed_pd, prior_uni_params,
  noise_sigma = sqrt(0.1), alpha = 1.0, sigma_y = 0.1
)

p_prior_inf <- plot_intensity(prior_inf_data, title = "Informative Prior")
p_prior_uni <- plot_intensity(prior_uni_data, title = "Uniform Prior")
p_post_inf <- plot_intensity(posterior_inf_data, observed = observed_pd, title = "Posterior (Informative)")
p_post_uni <- plot_intensity(posterior_uni_data, observed = observed_pd, title = "Posterior (Uniform)")

(p_prior_inf | p_prior_uni) / (p_post_inf | p_post_uni) +
  plot_layout(guides = "collect")
```

<!-- ## Bottleneck & Wasserstein Distance {.smaller} -->

<!-- For 2 Persistence Diagrams $X$ & $Y$: -->

<!-- :::{.fragment} -->
<!-- - **Bottleneck Distance**:   -->
<!--   Measures the maximum distance between matched points in persistence diagrams (PDs).   -->
<!--     - Sensitive to outliers (dominated by the largest mismatch).   -->

<!--   $$ -->
<!--   W_\infty(X,Y) = \inf_\limits{\eta: X \rightarrow{} Y} \sup_\limits{x \in X} \|x - \eta(x)\|_\infty -->
<!--   $$ -->
<!-- ::: -->

<!-- :::{.fragment} -->
<!-- - **Wasserstein Distance**:   -->
<!--   Measures the "work" required to transform one PD into another.   -->
<!--     - Robust to outliers (aggregates all mismatches).   -->

<!-- $$ -->
<!-- W_q(X,Y) = \left[ \inf_\limits{\eta: X \rightarrow{} Y} \sum_\limits{x \in X} \|x - \eta(x)\|^p_\infty \right]^{1/p} -->
<!-- $$ -->

<!-- ::: -->

<!-- :::{.notes} -->
<!-- - $\eta$ is a bijection -->
<!-- - $L_\infty = \max{\{|x_1-y_1|, |x_2 - y_2|, \ldots \}}$ -->
<!-- - **Bottleneck**: Useful for detecting *critical* topological features (e.g., a single hole vs. no holes).   -->
<!--     - supremum of distances between corresponding points for each. THen infimum over all bijections. -->
<!-- - **Wasserstein**: Better for *global* comparisons (e.g., distributions of holes in similar shapes). -->
<!--     - sum of qth powers of the $L_\infty$ distances, and again, minimizing over the bijections -->
<!-- - Bottleneck is Wasserstein as $q \rightarrow{} \infty$ -->
<!-- - For each PD, we are adding infinite points along diagonal -->
<!-- ::: -->

## Bottleneck & Wasserstein Distance

```{r, fig.dim=c(10,6)}
rips_data1 <- ripserr::vietoris_rips(p1) |> as.matrix()
rips_data2 <- ripserr::vietoris_rips(p2) |> as.matrix()

bottleneck_dist <- TDA::bottleneck(rips_data1, rips_data2)
wasserstein_dist <- TDA::wasserstein(rips_data1, rips_data2, p = 2)

rips_data_combined <- bind_rows(
  rips_data1 |> as_tibble() |> mutate(dataset = "Larger Circle"),
  rips_data2 |> as_tibble() |> mutate(dataset = "Smaller Circle")
)

ggplot(rips_data_combined) +
  stat_persistence(
    aes(start = birth, end = death,
        color = factor(dimension), shape = factor(dimension)),
    size = 2.5, alpha = .75
  ) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", alpha = 0.6) +
  scale_color_viridis_d(end = .7, name = "Dimension") +
  scale_shape_discrete(name = "Dimension") +
  labs(title = "Persistence Diagram") +
  facet_wrap(~ dataset, ncol = 2) +
  coord_cartesian(ylim = c(0, 1.65), xlim = c(0, 1.65)) +
  theme(legend.position = "bottom")
```

:::{.notes}
$\color{green}{\text{For dimension 1, loops/hole of circle}}$

- Bottleneck: `r bottleneck_dist |> round(4)`
- Wasserstein: p=2 `r wasserstein_dist |> round(4)`

- For dimension 0:
    - Bottleneck: `r TDA::bottleneck(rips_data1, rips_data2, dimension = 0) |> round(4)`
    - Wasserstein: p=2 `r TDA::wasserstein(rips_data1, rips_data2, p = 2, dimension = 0) |> round(4)`
:::

## Cubical Complexes



# Analysis {#sec-Analysis}

# Results/Future Work {#sec-Results}

# References {#sec-References}

## References

::: {#refs .smaller}
:::
