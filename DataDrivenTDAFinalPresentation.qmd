---
title: "The Shape of Digits"
subtitle: "A Bayesian Topological Data Analytic Approach to Classification of Handwritten Digits"
authors:
  - name: Thomas Reinke
    affiliation: 
      - name: Baylor University
        department: Statistical Science
        # city: Waco
        # state: TX
        # country: US
        url: https://www.baylor.edu
    # email: thomas_reinke1@baylor.edu
  - name: Theophilus A. Bediako
    affiliation: 
      - name: Baylor University
        department: Statistical Science
        # city: Waco
        # state: TX
        # country: US
        url: https://www.baylor.edu
  - name: Daniel Lim
    affiliation: 
      - name: Baylor University
        department: Statistical Science
        # city: Waco
        # state: TX
        # country: US
        url: https://www.baylor.edu
date: today
date-format: "MMMM D, YYYY"
format: 
  revealjs:
    theme: 
      - quarto-assets/baylor-theme.scss
    smaller: false
    scrollable: false
    show-slide-number: all
    toc: false
    toc-depth: 1
    preview-links: true
    slide-number: c/t
    multiplex: false
    embed-resources: true
    auto-animate: true
    #footer: "Thomas Reinke"
bibliography: references.bibtex
lightbox:
  match: auto
  effect: fade
  desc-position: bottom
  loop: true
logo: "quarto-assets/baylor.png"
license: "CC BY-NC"
copyright: 
  holder: Thomas Reinke
  year: 2025
editor: 
  markdown: 
    wrap: 72
fig-width: 15
---

```{r, setup}
#| include: false
#| message: false
library(quarto)
library(knitr)
library(tidyverse)
library(conflicted)
library(janitor)
library(ggtda)
# library(TDAvis)
library(patchwork)
library(gganimate)
library(ggforce)
library(simplextree) 
library(gifski)
library(magick)  
library(ripserr)
library(reshape2)
# remotes::install_github("maroulaslab/BayesTDA") Use this if package ‘BayesTDA’ is not available for this version of R
library(BayesTDA)
library(TDAstats)
library(mvtnorm)
library(kableExtra)
library(plotly)
library(DiagrammeR)
library(transport)
library(TDA)
library(RColorBrewer)
library(Rtsne)
library(keras)
conflicted::conflict_prefer("filter", "dplyr")
conflicted::conflict_prefer("select", "dplyr")
conflicted::conflicts_prefer(ggtda::geom_simplicial_complex)
conflicted::conflicts_prefer(plotly::layout)
knitr::opts_chunk$set(
  comment = "#>",
  message = FALSE,
  warning = FALSE,
  cache = FALSE,
  echo = FALSE,
  tidy.opts = list(width.cutoff = 100),
  tidy = FALSE,
  fig.align = "center"
)
ggplot2::theme_set(ggplot2::theme_minimal())
ggplot2::theme_update(panel.grid.minor = ggplot2::element_blank())

#------------------------------------------------------------#
```

::: {.content-hidden}
$$
{{< include quarto-assets/_macros.tex >}}
$$
:::


```{r}
#--------------------------------------------------------
#----------Load & Preprocess Data------------------------
#--------------------------------------------------------
mnist <- readRDS(file = "mnist_dataset")

train <- mnist$train
test <- mnist$test

train_images <- train$images
train_labels <- as.factor(train$labels)  

test_images <- test$images  
test_labels <- as.factor(test$labels)

train_images <- train_images / 255
test_images <- test_images / 255

train_images_list <- lapply(1:nrow(train_images), function(i) {
  matrix(train_images[i, ], nrow = 28) |> t()
})

test_images_list <- lapply(1:nrow(test_images), function(i) {
  matrix(test_images[i, ], nrow = 28) |> t()
})

plot_digit <- \(image_list = train_images_list, image_index = NULL, image_df = NULL, melted = FALSE){
  if(!melted){
    image_df <- melt(image_list[image_index])
    colnames(image_df) <- c("y", "x", "value")
  }
  ggplot(image_df, aes(x = x, y = y, fill = value)) + 
    geom_raster() +
  scale_fill_gradient(low = "white", high = "black") +
  scale_y_reverse() + 
  coord_equal() +
  theme_void() + 
  theme(legend.position = "none") 
}

# plot_digit(image_index = 8)
# paste0("Label: ", train$labels[8])

binarize_images <- function(images_list, threshold = 0.5) {
  lapply(images_list, function(mat) {
    ifelse(mat < threshold, 0, 1)
  })
}

train_images_binarized <- binarize_images(train_images_list)
test_images_binarized <- binarize_images(test_images_list)

# plot_digit(image_index = 8) + plot_digit(train_images_binarized, image_index = 8)

#------------------------------------------------------------#
```

# Contents

1. [MNIST Overview](#sec-MNIST)
<!-- 1. [Machine Learning/Classification Methods](#sec-MLMethod) -->
<!-- 1. [Intro to TDA](#sec-TDAIntro) -->
<!-- 1. [TDA Methodology](#sec-TDAMethod) -->
1. [Analysis](#sec-Analysis)
1. [Results/Future Work](#sec-Results)
1. [References](#sec-References)


# Exploratory Data Analysis


## Distribution of training labels
```{r dist_labels, fig.dim=c(10,6)}
#--------------------------------------------------------
#----------Distribution of training labels---------------
#--------------------------------------------------------
#first run the qmd file
train_df <- train$images |> as.data.frame()
train_df$labels <- train$labels |> as.factor()

ggplot(train_df, aes(labels, fill = labels))+
  geom_bar()+
  labs(fill = "digit")

```

## Pixel intensity representation of the training set
```{r pixel_intensity, fig.dim=c(10,6)}
#----------------------------------------------------------
#-----------Pixel intensity representation-----------------
#----------------------------------------------------------
train_df_long <- train_df |>
  pivot_longer(-labels, names_to  = "covariate", values_to = "intensity")
ggplot(train_df_long) +
  geom_histogram(
    aes(intensity),
    bins = 30,
    fill = "#2C7FB8",
    color = "white",
    alpha = 0.8
  )

```

## Training data in 2D 
```{r tnse, fig.dim=c(10,6)}
#------------------------------------------------------------
#----------------distribution of mnist training--------------
#------------------------------------------------------------
#normalized pixel values and apply t-SNE
# tsne_results <- Rtsne(
#   train_images,
#   dim = 2,
#   perplexity = 30,
#   max_iter = 1000
# )
# df_tsne <- tibble(Dim1 = tsne_results$Y[, 1],
#                   Dim2 = tsne_results$Y[, 2],
#                   digit = train_df$labels)
#
# saveRDS(df_tsne, "df_tsne")
df_tsne <- readRDS("df_tsne")
ggplot(df_tsne, aes(Dim1, Dim2, color = digit)) +
  geom_point(alpha = .9)
```

We adopt t-distributed Stochastic Neighbor Embedding(t-SNE) to represent the data in 2D.(see  details ![here](https://rpubs.com/TusVasMit/T-SNEExploration))


# TDA Workflow

```{tikz}
%| echo: false

\usetikzlibrary{positioning, arrows.meta}

\begin{tikzpicture}[
    node distance = 1.2cm and 2cm,
    auto,
    every node/.style={
        draw,
        thick,
        rounded corners,
        align=center,
        minimum height=1.2cm
    },
    arrow/.style={->, >=Stealth}
]

% Define nodes for the flowchart
\node (train) {Train Data \\ 60,000 \\ 28x28};
\node (test) [below=4.5cm of train] {Test Data \\ 10,000 \\ 28x28};

\node (train1) [right=.5cm of train] {EB Set 5\%};
\node (train2) [below=1.75cm of train1] {Training Set 95\%};

\node (priors) [right=2.5cm of train1] {Priors, \\2/class};
\node (OPD0) [below=.5cm of priors] {Observed PD's for 0};
\node (PDdots) [below=0.1cm of OPD0] {$\vdots$};
\node (OPD9) [below=.1cm of PDdots] {Observed PD's for 9};

\node (update) [right=1.5cm of PDdots, draw=none, scale=2] {$\odot$};
\node (priordots) [right=2.75cm of update] {$\vdots$};
\node (prior0) [above=.1cm of priordots] {Uniformitive Prior for 0 \\ Relatively informitive Prior for 0};
\node (prior9) [below=0.1cm of priordots] {Uniformitive Prior for 9 \\ Relatively informitive Prior for 9};

\node (equals) [right=2.5cm of priordots, draw=none, scale=2] {=};

\node (postdots) [right=1.25cm of equals] {$\vdots$};
\node (post0) [above=.1cm of postdots] {2 Posteriors for 0};
\node (post9) [below=.1cm of postdots] {2 Posteriors for 9};

\node (testDist) [right=2.5cm of postdots] {10 Uninf Distances \\ 10 Rel Inf Distances};
\node (testPD) [below=1.5cm of testDist] {Test PD};

\node (classify) [right=.5cm of testDist] {Classify as $\mathrm{argmin}$ Distances};

% Draw edges between nodes
\draw [arrow] (train) -- (train1);
\draw [arrow] (train) |- (train2);

\draw [arrow] (train1) -- (priors);
\draw [arrow] (priors) -| (prior0);

\draw [arrow] (train2) -- (OPD0);
\draw [arrow] (train2) -- (PDdots);
\draw [arrow] (train2) -- (OPD9);

%\draw [arrow, bend left=45] (OPD0) to (post0);
\draw [arrow] (OPD0) -- ++(0,1cm) -- ++(2cm,0) -| (post0);
\draw [arrow] (OPD9) -- ++(0,-1cm) -- ++(2cm,0) -| (post9);
\draw [arrow] (prior0) -- (post0);
\draw [arrow] (prior9) -- (post9);

\draw [arrow] (post0) -- (testDist);
\draw [arrow] (postdots) -- (testDist);
\draw [arrow] (post9) -- (testDist);
\draw [arrow] (testPD) -- (testDist);
\draw [arrow] (test) -- (testPD);

\draw [arrow] (testDist) -- (classify);
%\draw [arrow, bend left] (posterior) to node [pos=0.5, above] {EB} (proj);

\end{tikzpicture}
```





# Machine Learning Methods

## Neural networks

We use feedforward neural network which has the following structure

- input layer: consists of neurons that receives the input data each neuron in the input layer represents a feature of the input data
- hidden layer: one or more hidden layers placed between the input and output layers,  responsible for capturing complex patterns
- output layer: final output of the network. Here, the number of neurons represents the number of digits

## NN with regularization

:::{.fragement}
- Depending on a model, a network can have more weights than the size of training data. This leads to overfitting.
- We considered two approaches to overfitting
  - Dropout learning: think of RF, randomly removes a fraction of the units in a layer during model fitting
  - Regularization: impose penalties on parameters like lasso, ridge etc
:::


## Specific NN models considered

- NN with dropout regularization
- NN with ridge regularization
- NN with lasso regularization 


## Multinomial logistic regression

Multinomial logistic regression is equivalent to a NN with just input and output layers. There are no hidden layers. 

- Output layer with softmax



# Analysis {#sec-Analysis}

## To be edited 
Let’s train the network for 30 epochs, using a default batch size of 32. This means that images from the training set will be presented to the network in batches of 32 at a time, and for each batch, the SGD algorithm will update the network’s weights by an appropriate amount. Then another batch of 32 images will be presented, and so on, until all 60,000 training images in the dataset have been processed, which constitutes one epoch of training. This entire cycle will be repeated for 30 epochs. As training proceeds, the network’s error (loss) on both the training and testing/validation sets will be shown on the left graph, and the accuracy on each of these sets will be shown on the right graph. The accuracy is simply the fraction of input images that the network classifies correctly. A classification is considered correct if the largest output value on the output layer corresponds to the target classification


# Results/Future Work {#sec-Results}

```{r}
#run the Mnist ML method code first, and take results from there
# ml_results <- tibble(
#   method = c("multinomial", "dropout nn", "ridge nn", "lasso no"),
#   accuracy = c(mlogit_acc, nn_dropout_accu, nn_ridge_accu, nn_lasso_accu)
# ) 
# saveRDS(ml_results, "ml_results")

ml_results <- readRDS("ml_results") #still a tibble so easy to edit

ml_results |> 
  kable(caption = "ML results", digits = 4)
# tibble(
#   method = c("multinomial", "dropout nn", "ridge nn", "lasso no"),
#   accuracy = c(mlogit_acc, nn_dropout_accu, nn_ridge_accu, nn_lasso_accu)
# )
```



# References {#sec-References}

## References

::: {#refs .smaller}
:::
